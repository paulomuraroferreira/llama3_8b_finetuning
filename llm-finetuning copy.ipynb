{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%%capture\n","\n","!pip install bitsandbytes accelerate peft trl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:40:53.531106Z","iopub.status.busy":"2024-04-21T10:40:53.530787Z","iopub.status.idle":"2024-04-21T10:41:12.381544Z","shell.execute_reply":"2024-04-21T10:41:12.380576Z","shell.execute_reply.started":"2024-04-21T10:40:53.531076Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/Projects/llama3_8b_finetuning/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import time\n","from random import randrange, sample, seed\n","\n","import torch\n","import os\n","from datasets import load_dataset\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","\n","seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:12.694374Z","iopub.status.busy":"2024-04-21T10:41:12.694052Z","iopub.status.idle":"2024-04-21T10:41:12.700121Z","shell.execute_reply":"2024-04-21T10:41:12.699090Z","shell.execute_reply.started":"2024-04-21T10:41:12.694346Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using flash attention 2: True\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: flash-attn in ./venv/lib/python3.10/site-packages (2.6.1)\n","Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from flash-attn) (2.3.1)\n","Requirement already satisfied: einops in ./venv/lib/python3.10/site-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\n","Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2024.5.0)\n","Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n","Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.15.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2.20.5)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)\n","Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\n","Requirement already satisfied: triton==2.3.1 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2.3.1)\n","Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (1.13.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["use_flash_attention2 = False\n","\n","# Replace attention with flash attention \n","if torch.cuda.get_device_capability()[0] >= 8:\n","    use_flash_attention2 = True\n","\n","print(f\"Using flash attention 2: {use_flash_attention2}\")\n","\n","if use_flash_attention2:\n","    !pip install flash-attn --no-build-isolation --upgrade"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-21T10:41:12.713607Z","iopub.status.busy":"2024-04-21T10:41:12.713350Z","iopub.status.idle":"2024-04-21T10:41:20.191567Z","shell.execute_reply":"2024-04-21T10:41:20.190635Z","shell.execute_reply.started":"2024-04-21T10:41:12.713585Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 2429 examples [00:00, 53169.76 examples/s]\n"]}],"source":["from datasets import load_dataset\n","from utils import Variables\n","dataset = load_dataset(\"json\", data_files=Variables.INSTRUCTION_DATASET_JSON_PATH, split=\"train\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:20.192868Z","iopub.status.busy":"2024-04-21T10:41:20.192575Z","iopub.status.idle":"2024-04-21T10:41:20.197907Z","shell.execute_reply":"2024-04-21T10:41:20.196871Z","shell.execute_reply.started":"2024-04-21T10:41:20.192842Z"},"trusted":true},"outputs":[],"source":["def format_instruction(sample):\n","\treturn f\"\"\"    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{sample['Instruction']}\n","\n","### Input:\n","{sample['Input']}\n","\n","### Response:\n","{sample['Output']}\n","\"\"\""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n","# Load the entire dataset\n","dataset = load_dataset(\"json\", data_files=Variables.INSTRUCTION_DATASET_JSON_PATH)\n","\n","# Split the dataset into training and testing sets\n","train_test_split = dataset['train'].train_test_split(test_size=0.2)\n","\n","# Create a DatasetDict to hold the splits\n","dataset_dict = DatasetDict({\n","    'train': train_test_split['train'],\n","    'test': train_test_split['test']\n","})\n","\n","# Now you have separate training and testing sets\n","train_dataset = dataset_dict['train']\n","test_dataset = dataset_dict['test']"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:20.199512Z","iopub.status.busy":"2024-04-21T10:41:20.199159Z","iopub.status.idle":"2024-04-21T10:41:20.451315Z","shell.execute_reply":"2024-04-21T10:41:20.450321Z","shell.execute_reply.started":"2024-04-21T10:41:20.199481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Describe the KV cache mechanism in the context of large language models.\n","\n","### Input:\n","Figure 2 illustrates the KV cache mechanism in both static and real-time editing settings for large language models.\n","\n","### Response:\n","The KV cache stores precomputed Key/Value pairs that the model leverages to generate predictions in the static setting, and updates in real-time editing settings to maintain accurate information.\n","\n","\n","\n"]}],"source":["print(format_instruction(train_dataset[randrange(len(train_dataset))]))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards: 100%|██████████| 4/4 [03:04<00:00, 46.13s/it]\n","Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["('/root/Projects/llama3_8b_finetuning/models/original_llama3_model/tokenizer_config.json',\n"," '/root/Projects/llama3_8b_finetuning/models/original_llama3_model/special_tokens_map.json',\n"," '/root/Projects/llama3_8b_finetuning/models/original_llama3_model/tokenizer.json')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","import os\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Hugging Face model id\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","# model_id = \"mistralai/Mistral-7B-v0.1\"\n","\n","# BitsAndBytesConfig int-4 config \n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id, \n","    quantization_config=bnb_config, \n","    use_cache=False, \n","    device_map=\"auto\",\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n","    attn_implementation=\"flash_attention_2\" if use_flash_attention2 else \"sdpa\"\n",")\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Specify the directory where you want to save the model and tokenizer\n","save_directory = Variables.ORIGINAL_MODEL_PATH\n","\n","# Save the model\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(save_directory)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:19.246738Z","iopub.status.busy":"2024-04-21T10:46:19.246447Z","iopub.status.idle":"2024-04-21T10:46:19.274061Z","shell.execute_reply":"2024-04-21T10:46:19.273257Z","shell.execute_reply.started":"2024-04-21T10:46:19.246713Z"},"trusted":true},"outputs":[],"source":["# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        r=64,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","            \"q_proj\",\n","            \"k_proj\",\n","            \"v_proj\",\n","            \"o_proj\",\n","            \"gate_proj\", \n","            \"up_proj\", \n","            \"down_proj\",\n","        ]\n",")\n","\n","# Prepare model for training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:19.275606Z","iopub.status.busy":"2024-04-21T10:46:19.275240Z","iopub.status.idle":"2024-04-21T10:46:21.696131Z","shell.execute_reply":"2024-04-21T10:46:21.695089Z","shell.execute_reply.started":"2024-04-21T10:46:19.275574Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/Projects/llama3_8b_finetuning/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/root/Projects/llama3_8b_finetuning/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/root/Projects/llama3_8b_finetuning/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","Generating train split: 143 examples [00:00, 862.48 examples/s]\n"]}],"source":["args = TrainingArguments(\n","    output_dir= Variables.FINE_TUNED_MODEL_PATH,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,#6 if use_flash_attention2 else 2, # you can play with the batch size depending on your hardware\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=10,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=use_flash_attention2,\n","    fp16=not use_flash_attention2,\n","    tf32=use_flash_attention2,\n","    max_grad_norm=0.3,\n","    warmup_steps=5,\n","    lr_scheduler_type=\"linear\",\n","    disable_tqdm=False,\n","    report_to=\"none\"\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    max_seq_length=2048,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_instruction, \n","    args=args,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:22.671049Z","iopub.status.busy":"2024-04-21T10:46:22.670741Z","iopub.status.idle":"2024-04-21T11:13:51.648998Z","shell.execute_reply":"2024-04-21T11:13:51.648127Z","shell.execute_reply.started":"2024-04-21T10:46:22.671021Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/Projects/llama3_8b_finetuning/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 03:13, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# train\n","trainer.train()\n","\n","# save model\n","trainer.save_model()"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T11:13:51.651118Z","iopub.status.busy":"2024-04-21T11:13:51.650526Z","iopub.status.idle":"2024-04-21T11:14:54.059744Z","shell.execute_reply":"2024-04-21T11:14:54.058777Z","shell.execute_reply.started":"2024-04-21T11:13:51.651090Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.90s/it]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Path to finetuned model\n","finetuned_model_dir=Variables.FINE_TUNED_MODEL_PATH\n","\n","# Load finetuned LLM model and tokenizer\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    finetuned_model_dir,\n","    low_cpu_mem_usage=True,\n","    torch_dtype=torch.float16,\n","    load_in_4bit=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n","\n","instruction = {\n","    \"Instruction\": \"Answer the following question\",\n","    \"Input\": \"Explain the significance of LoRA-Guard's performance in cross-domain evaluation.\",\n","    \"Output\": \"\"\n","}"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def ask_question(instruction, temperature=0.5):\n","    \n","    prompt = format_instruction(instruction)\n","\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","    start_time = time.time()\n","    with torch.inference_mode():\n","        outputs = model.generate(input_ids=input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=True, top_p=0.5,temperature=temperature)\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","    output_length = len(outputs[0])-len(input_ids[0])\n","\n","    output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n","\n","    #print(f\"\\nInstruction generated from finetuned model | Inference time - {total_time:.2f}s:\\n\")\n","\n","    return output"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["def parse_output(text):    \n","\n","    # Split the text at the word \"Response\"\n","    parts = text.split(\"Response:\", 1)\n","\n","    # Check if \"Response\" is in the text and get the part after it\n","    if len(parts) > 1:\n","        response_text = parts[1].strip()\n","    else:\n","        response_text = \"\"\n","\n","    return response_text\n","\n","parse_output(ask_question(test_dataset[2]))"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","from nltk.translate.bleu_score import corpus_bleu\n","from rouge_score import rouge_scorer\n","from transformers import BertTokenizer, BertForMaskedLM, BertModel\n","from bert_score import BERTScorer\n","\n","def calculate_bleu_score(machine_results, reference_texts):\n","    bleu_score = corpus_bleu([[ref.split()] for ref in reference_texts], [gen.split() for gen in machine_results])\n","    print(f'BLEU Score: {bleu_score}')\n","\n","def calculate_rouge_scores(generated_answers, ground_truth):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n","    for gen, ref in zip(generated_answers, ground_truth):\n","        scores = scorer.score(gen, ref)\n","        total_rouge1 += scores['rouge1'].fmeasure\n","        total_rouge2 += scores['rouge2'].fmeasure\n","        total_rougeL += scores['rougeL'].fmeasure\n","    average_rouge1 = total_rouge1 / len(generated_answers)\n","    average_rouge2 = total_rouge2 / len(generated_answers)\n","    average_rougeL = total_rougeL / len(generated_answers)\n","    print(f'Average ROUGE-1: {average_rouge1}')\n","    print(f'Average ROUGE-2: {average_rouge2}')\n","    print(f'Average ROUGE-L: {average_rougeL}')"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["new_test_dataset = []\n","for dict_ in test_dataset:\n","    dict_['Output'] = ''\n","    new_test_dataset.append(dict_)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["generated_answer = parse_output(ask_question(new_test_dataset[0]))\n","ground_truth = test_dataset[0]['Output']"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average ROUGE-1: 0.020654044750430294\n","Average ROUGE-2: 0.0\n","Average ROUGE-L: 0.020654044750430294\n"]}],"source":["calculate_rouge_scores(generated_answer,ground_truth)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
