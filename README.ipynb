{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an end-to-end project including data ingestion, creation of instruction/answer pairs, fine-tuning, and evaluation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step - Data Scraping\n",
    "\n",
    "Start by installing the dependencies with \n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "In order to find data for the fine-tuning, Arxiv was scraped for LLM papers published after the Llama 3 release date.\n",
    "\n",
    "The Selenium scraping code can be found in `llama3_8b_finetuning/arxiv_scraping/Arxiv_pdfs_download.py`. (The webdriver must be downloaded before this script execution).\n",
    "\n",
    "The scraping code takes the papers on the first Arxiv page and downloads them into the `llama3_8b_finetuning/data/pdfs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Step - Instructions/Answer Pairs Creation\n",
    "\n",
    "The code for this step can be found at `/llama3_8b_finetuning/creating_instruction_dataset.py`\n",
    "\n",
    "The text content from the downloaded papers was parsed using Langchain's `PyPDFLoader`. Then, the text was sent into the Llama 3 70B model via Grok. Grok was chosen due to its speed and small cost. Also, it must be noted that the Llama 3 user license only allows its use for training/fine tuning Llama LLMs. Therefore, we wouldn't be able to use Llama 3 for create instructions/answer pairs for other models, even open-source ones or for non-commercial use.,\n",
    "\n",
    "The prompt for the pairs creation are on utils file, and it can also be seen below: \n",
    "\n",
    "'''\n",
    "\n",
    "    You are a highly intelligent and knowledgeable assistant tasked with generating triples of instruction, input, and output from academic papers related to Large Language Models (LLMs). Each triple should consist of:\n",
    "\n",
    "    Instruction: A clear and concise task description that can be performed by an LLM.\n",
    "    Input: A sample input that corresponds to the instruction.\n",
    "    Output: The expected result or answer when the LLM processes the input according to the instruction.\n",
    "    Below are some example triples:\n",
    "\n",
    "    Example 1:\n",
    "\n",
    "    Instruction: Summarize the following abstract.\n",
    "    Input: \"In this paper, we present a new approach to training large language models by incorporating a multi-task learning framework. Our method improves the performance on a variety of downstream tasks.\"\n",
    "    Output: \"A new multi-task learning framework improves the performance of large language models on various tasks.\"\n",
    "    Example 2:\n",
    "\n",
    "    Instruction: Provide a brief explanation of the benefits of using multi-task learning for large language models.\n",
    "    Input: \"Multi-task learning allows a model to learn from multiple related tasks simultaneously, which can lead to better generalization and performance improvements across all tasks. This approach leverages shared representations and can reduce overfitting.\"\n",
    "    Output: \"Multi-task learning helps large language models generalize better and improve performance by learning from multiple related tasks simultaneously.\"\n",
    "    Now, generate similar triples based on the provided text from academic papers related to LLMs:\n",
    "\n",
    "    Source Text\n",
    "    (Provide the text from the academic papers here)\n",
    "\n",
    "    Generated Triples\n",
    "    Triple 1:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 2:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 3:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "\n",
    "\n",
    "Finally, the instructions are saved on `llama3_8b_finetuning/data/arvix_instruction_dataset.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Step - Fine Tuning\n",
    "\n",
    "The code for this step can be found on `/llama3_8b_finetuning/model_trainer.py`\n",
    "\n",
    "\n",
    "First we load the instructions/answer pairs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(self):\n",
    "dataset = load_dataset(\"json\", data_files=self.data_path)\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "return dataset_dict['train'], dataset_dict['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
