{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an end-to-end project including data ingestion, creation of instruction/answer pairs, fine-tuning, and evaluation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step - Data Scraping\n",
    "\n",
    "Start by installing the dependencies with \n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "In order to find data for the fine-tuning, Arxiv was scraped for LLM papers published after the Llama 3 release date.\n",
    "\n",
    "The Selenium scraping code can be found in `llama3_8b_finetuning/arxiv_scraping/Arxiv_pdfs_download.py`. (The webdriver must be downloaded before this script execution).\n",
    "\n",
    "The scraping code takes the papers on the first Arxiv page and downloads them into the `llama3_8b_finetuning/data/pdfs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Step - Instructions/Answer Pairs Creation\n",
    "\n",
    "The code for this step can be found at `/llama3_8b_finetuning/creating_instruction_dataset.py`\n",
    "\n",
    "The text content from the downloaded papers was parsed using Langchain's `PyPDFLoader`. Then, the text was sent into the Llama 3 70B model via Grok. Grok was chosen due to its speed and small cost. Also, it must be noted that the Llama 3 user license only allows its use for training/fine tuning Llama LLMs. Therefore, we wouldn't be able to use Llama 3 for create instructions/answer pairs for other models, even open-source ones or for non-commercial use.\n",
    "\n",
    "The prompt for the pairs creation are on the utils file, and it can also be seen below: \n",
    "\n",
    "'''\n",
    "\n",
    "    You are a highly intelligent and knowledgeable assistant tasked with generating triples of instruction, input, and output from academic papers related to Large Language Models (LLMs). Each triple should consist of:\n",
    "\n",
    "    Instruction: A clear and concise task description that can be performed by an LLM.\n",
    "    Input: A sample input that corresponds to the instruction.\n",
    "    Output: The expected result or answer when the LLM processes the input according to the instruction.\n",
    "    Below are some example triples:\n",
    "\n",
    "    Example 1:\n",
    "\n",
    "    Instruction: Summarize the following abstract.\n",
    "    Input: \"In this paper, we present a new approach to training large language models by incorporating a multi-task learning framework. Our method improves the performance on a variety of downstream tasks.\"\n",
    "    Output: \"A new multi-task learning framework improves the performance of large language models on various tasks.\"\n",
    "    Example 2:\n",
    "\n",
    "    Instruction: Provide a brief explanation of the benefits of using multi-task learning for large language models.\n",
    "    Input: \"Multi-task learning allows a model to learn from multiple related tasks simultaneously, which can lead to better generalization and performance improvements across all tasks. This approach leverages shared representations and can reduce overfitting.\"\n",
    "    Output: \"Multi-task learning helps large language models generalize better and improve performance by learning from multiple related tasks simultaneously.\"\n",
    "    Now, generate similar triples based on the provided text from academic papers related to LLMs:\n",
    "\n",
    "    Source Text\n",
    "    (Provide the text from the academic papers here)\n",
    "\n",
    "    Generated Triples\n",
    "    Triple 1:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 2:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 3:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "\n",
    "\n",
    "Finally, the instructions are saved on `llama3_8b_finetuning/data/arvix_instruction_dataset.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Step - Fine Tuning\n",
    "\n",
    "The code for this step can be found on `/llama3_8b_finetuning/model_trainer.py`\n",
    "\n",
    "\n",
    "First we load the instructions/answer pairs, split them into test and train dataset, and format \n",
    "into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_and_split_dataset(self):\n",
    "        dataset = load_dataset(\"json\", data_files=self.data_path)\n",
    "        train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        return dataset_dict['train'], dataset_dict['test']\n",
    "\n",
    "    @staticmethod\n",
    "    def format_instruction(sample):\n",
    "        return f\"\"\"\n",
    "        Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {sample['Instruction']}\n",
    "\n",
    "        ### Input:\n",
    "        {sample['Input']}\n",
    "\n",
    "        ### Response:\n",
    "        {sample['Output']}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the class that loads the model and tokenizer from huggingface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    def __init__(self, model_id, use_flash_attention2, hf_token):\n",
    "        self.model_id = model_id\n",
    "        self.use_flash_attention2 = use_flash_attention2\n",
    "        self.hf_token = hf_token\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",
    "        )\n",
    "    \n",
    "    def load_model_and_tokenizer(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id, \n",
    "            quantization_config=self.bnb_config, \n",
    "            use_cache=False, \n",
    "            device_map=\"auto\",\n",
    "            token=self.hf_token,  \n",
    "            attn_implementation=\"flash_attention_2\" if self.use_flash_attention2 else \"sdpa\"\n",
    "        )\n",
    "        model.config.pretraining_tp = 1\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id,\n",
    "            token=self.hf_token\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        return model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
