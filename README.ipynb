{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an end-to-end project including data ingestion, creation of instruction/answer pairs, fine-tuning, and evaluation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step - Data Scraping\n",
    "\n",
    "Start by installing the dependencies with \n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "In order to find data for the fine-tuning, Arxiv was scraped for LLM papers published after the Llama 3 release date.\n",
    "\n",
    "The Selenium scraping code can be found in `llama3_8b_finetuning/arxiv_scraping/Arxiv_pdfs_download.py`. (The webdriver must be downloaded before this script execution).\n",
    "\n",
    "The scraping code takes the papers on the first Arxiv page and downloads them into the `llama3_8b_finetuning/data/pdfs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Step - Instructions/Answer Pairs Creation\n",
    "\n",
    "The code for this step can be found at `/llama3_8b_finetuning/creating_instruction_dataset.py`\n",
    "\n",
    "The text content from the downloaded papers was parsed using Langchain's `PyPDFLoader`. Then, the text was sent into the Llama 3 70B model via Grok. Grok was chosen due to its speed and small cost. Also, it must be noted that the Llama 3 user license only allows its use for training/fine tuning Llama LLMs. Therefore, we wouldn't be able to use Llama 3 for create instructions/answer pairs for other models, even open-source ones or for non-commercial use.\n",
    "\n",
    "The prompt for the pairs creation are on the utils file, and it can also be seen below: \n",
    "\n",
    "'''\n",
    "\n",
    "    You are a highly intelligent and knowledgeable assistant tasked with generating triples of instruction, input, and output from academic papers related to Large Language Models (LLMs). Each triple should consist of:\n",
    "\n",
    "    Instruction: A clear and concise task description that can be performed by an LLM.\n",
    "    Input: A sample input that corresponds to the instruction.\n",
    "    Output: The expected result or answer when the LLM processes the input according to the instruction.\n",
    "    Below are some example triples:\n",
    "\n",
    "    Example 1:\n",
    "\n",
    "    Instruction: Summarize the following abstract.\n",
    "    Input: \"In this paper, we present a new approach to training large language models by incorporating a multi-task learning framework. Our method improves the performance on a variety of downstream tasks.\"\n",
    "    Output: \"A new multi-task learning framework improves the performance of large language models on various tasks.\"\n",
    "    Example 2:\n",
    "\n",
    "    Instruction: Provide a brief explanation of the benefits of using multi-task learning for large language models.\n",
    "    Input: \"Multi-task learning allows a model to learn from multiple related tasks simultaneously, which can lead to better generalization and performance improvements across all tasks. This approach leverages shared representations and can reduce overfitting.\"\n",
    "    Output: \"Multi-task learning helps large language models generalize better and improve performance by learning from multiple related tasks simultaneously.\"\n",
    "    Now, generate similar triples based on the provided text from academic papers related to LLMs:\n",
    "\n",
    "    Source Text\n",
    "    (Provide the text from the academic papers here)\n",
    "\n",
    "    Generated Triples\n",
    "    Triple 1:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 2:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "    Triple 3:\n",
    "\n",
    "    Instruction:\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "\n",
    "\n",
    "Finally, the instructions are saved on `llama3_8b_finetuning/data/arvix_instruction_dataset.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Step - Fine Tuning\n",
    "\n",
    "The code for this step can be found on `/llama3_8b_finetuning/model_trainer.py`\n",
    "\n",
    "\n",
    "First we load the instructions/answer pairs, split them into test and train dataset, and format \n",
    "into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_and_split_dataset(self):\n",
    "        dataset = load_dataset(\"json\", data_files=self.data_path)\n",
    "        train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        return dataset_dict['train'], dataset_dict['test']\n",
    "\n",
    "    @staticmethod\n",
    "    def format_instruction(sample):\n",
    "        return f\"\"\"\n",
    "        Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {sample['Instruction']}\n",
    "\n",
    "        ### Input:\n",
    "        {sample['Input']}\n",
    "\n",
    "        ### Response:\n",
    "        {sample['Output']}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the class that loads the model and tokenizer from huggingface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    def __init__(self, model_id, use_flash_attention2, hf_token):\n",
    "        self.model_id = model_id\n",
    "        self.use_flash_attention2 = use_flash_attention2\n",
    "        self.hf_token = hf_token\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",
    "        )\n",
    "    \n",
    "    def load_model_and_tokenizer(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id, \n",
    "            quantization_config=self.bnb_config, \n",
    "            use_cache=False, \n",
    "            device_map=\"auto\",\n",
    "            token=self.hf_token,  \n",
    "            attn_implementation=\"flash_attention_2\" if self.use_flash_attention2 else \"sdpa\"\n",
    "        )\n",
    "        model.config.pretraining_tp = 1\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id,\n",
    "            token=self.hf_token\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the Trainer class and the training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer, train_dataset, peft_config, use_flash_attention2, output_dir):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.peft_config = peft_config\n",
    "        self.args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-4,\n",
    "            bf16=use_flash_attention2,\n",
    "            fp16=not use_flash_attention2,\n",
    "            tf32=use_flash_attention2,\n",
    "            max_grad_norm=0.3,\n",
    "            warmup_steps=5,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            disable_tqdm=False,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, self.peft_config)\n",
    "\n",
    "    def train_model(self, format_instruction_func):\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.train_dataset,\n",
    "            peft_config=self.peft_config,\n",
    "            max_seq_length=2048,\n",
    "            tokenizer=self.tokenizer,\n",
    "            packing=True,\n",
    "            formatting_func=format_instruction_func, \n",
    "            args=self.args,\n",
    "        )\n",
    "        trainer.train()\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the classes are called and the training starts.\n",
    "\n",
    "Note that the Llama models are gated, i.e., HuggingFace requires a token given after   \n",
    "the terms of the model are accepted and Meta approves the access (which is almost instantly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_handler = DatasetHandler(data_path=utils.Variables.INSTRUCTION_DATASET_JSON_PATH)\n",
    "train_dataset, test_dataset = dataset_handler.load_and_split_dataset()\n",
    "\n",
    "new_test_dataset = []\n",
    "for dict_ in test_dataset:\n",
    "    dict_['Output'] = ''\n",
    "    new_test_dataset.append(dict_)\n",
    "\n",
    "model_manager = ModelManager(\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    use_flash_attention2=True,\n",
    "    hf_token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "model, tokenizer = model_manager.load_model_and_tokenizer()\n",
    "model_manager.save_model_and_tokenizer(model, tokenizer, save_directory=utils.Variables.BASE_MODEL_PATH)\n",
    "model = model_manager.prepare_for_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    use_flash_attention2=True,\n",
    "    output_dir=utils.Variables.FINE_TUNED_MODEL_PATH\n",
    ")\n",
    "trained_model = trainer.train_model(format_instruction_func=dataset_handler.format_instruction)\n",
    "trained_model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Evaluation\n",
    "\n",
    "In order to evaluate the fine tuning results, we employed the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, which \n",
    "compares the overlap between two sets of text, in order to measure similarity between them.\n",
    "\n",
    "Specifically, we employ the rouge_scorer library to calculate Rouge1 and Rouge2, which measures the 1-gram and 2-gram overlap between the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return {'average_rouge1':average_rouge1,\n",
    "            'average_rouge2':average_rouge2,\n",
    "            'average_rougeL':average_rougeL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform this calculation, we take the instructions from the test dataset, pass them into both the base model  \n",
    "and the fine-tuned model, and compare it to the expected output from the instruction/answer dataset. \n",
    "\n",
    "The code for the evaluation can be found at `/llama3_8b_finetuning/model_evaluation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def loading_model(self, model_chosen='fine_tuned_model'):\n",
    "\n",
    "        if model_chosen == 'fine_tuned_model':\n",
    "            model_dir=utils.Variables.FINE_TUNED_MODEL_PATH\n",
    "            self.model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                model_dir,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                load_in_4bit=True,\n",
    "                )\n",
    "\n",
    "        elif model_chosen == 'base_model':\n",
    "            model_dir=utils.Variables.BASE_MODEL_PATH\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_dir,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                load_in_4bit=True,\n",
    "                )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    def ask_question(self, instruction, temperature=0.5, max_new_tokens = 1000):\n",
    "\n",
    "        prompt = format_instruction(instruction)\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(input_ids=input_ids, pad_token_id=self.tokenizer.eos_token_id, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.5,temperature=temperature)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        output_length = len(outputs[0])-len(input_ids[0])\n",
    "\n",
    "        self.output = self.tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rouge scores are as follows:\n",
    "    \n",
    "FINE-TUNED MODEL:\n",
    "\n",
    "`{'average_rouge1': 0.39997816307812206,\n",
    " 'average_rouge2': 0.2213826792342886,\n",
    " 'average_rougeL': 0.33508922374837047}`\n",
    "\n",
    "BASE MODEL: \n",
    "\n",
    "`{'average_rouge1': 0.2524191394349585,\n",
    " 'average_rouge2': 0.13402054342344535,\n",
    " 'average_rougeL': 0.2115590931984475}`  \n",
    "   \n",
    "     \n",
    "     \n",
    "Therefore, it can be seen that the perfomance for the fine-tuned model on the test dataset are \n",
    "significatly superior than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tooks quite a while to write this code and get it working. It was good to practice, but for everyday fine-tuning related    \n",
    "jobs, just use hugging face autotrainer hosted locally (https://github.com/huggingface/autotrain-advanced): "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
