{"cells":[{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["%%capture\n","\n","!pip install bitsandbytes accelerate peft trl\n","\n","use_flash_attention2 = False\n","\n","import torch\n","# Replace attention with flash attention \n","if torch.cuda.get_device_capability()[0] >= 8:\n","    use_flash_attention2 = True\n","\n","print(f\"Using flash attention 2: {use_flash_attention2}\")\n","\n","if use_flash_attention2:\n","    !pip install flash-attn --no-build-isolation --upgrade\n","\n","import torch\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/workspace/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'dotenv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n","import torch\n","from datasets import load_dataset, DatasetDict\n","from dotenv import load_dotenv\n","from random import seed\n","import os\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","from trl import SFTTrainer\n","import utils\n","\n","# Set seed for reproducibility\n","seed(42)\n","\n","class EnvironmentLoader:\n","    @staticmethod\n","    def load_env():\n","        load_dotenv()\n","\n","class DatasetHandler:\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","\n","    def load_and_split_dataset(self):\n","        dataset = load_dataset(\"json\", data_files=self.data_path)\n","        train_test_split = dataset['train'].train_test_split(test_size=0.2)\n","        dataset_dict = DatasetDict({\n","            'train': train_test_split['train'],\n","            'test': train_test_split['test']\n","        })\n","        return dataset_dict['train'], dataset_dict['test']\n","\n","    @staticmethod\n","    def format_instruction(sample):\n","        return f\"\"\"\n","        Below is an instruction that describes a task, paired with an input that provides further context. \n","        Write a response that appropriately completes the request.\n","\n","        ### Instruction:\n","        {sample['Instruction']}\n","\n","        ### Input:\n","        {sample['Input']}\n","\n","        ### Response:\n","        {sample['Output']}\n","        \"\"\"\n","\n","class ModelManager:\n","    def __init__(self, model_id, use_flash_attention2, hf_token):\n","        self.model_id = model_id\n","        self.use_flash_attention2 = use_flash_attention2\n","        self.hf_token = hf_token\n","        self.bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n","        )\n","    \n","    def load_model_and_tokenizer(self):\n","        model = AutoModelForCausalLM.from_pretrained(\n","            self.model_id, \n","            quantization_config=self.bnb_config, \n","            use_cache=False, \n","            device_map=\"auto\",\n","            token=self.hf_token,  \n","            attn_implementation=\"flash_attention_2\" if self.use_flash_attention2 else \"sdpa\"\n","        )\n","        model.config.pretraining_tp = 1\n","\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            self.model_id,\n","            token=self.hf_token\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer.padding_side = \"right\"\n","        \n","        return model, tokenizer\n","\n","    def save_model_and_tokenizer(self, model, tokenizer, save_directory):\n","        model.save_pretrained(save_directory)\n","        tokenizer.save_pretrained(save_directory)\n","    \n","    @staticmethod\n","    def prepare_for_training(model):\n","        return prepare_model_for_kbit_training(model)\n","\n","class Trainer:\n","    def __init__(self, model, tokenizer, train_dataset, peft_config, use_flash_attention2, output_dir):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.train_dataset = train_dataset\n","        self.peft_config = peft_config\n","        self.args = TrainingArguments(\n","            output_dir=output_dir,\n","            num_train_epochs=1,\n","            per_device_train_batch_size=4,\n","            gradient_accumulation_steps=4,\n","            gradient_checkpointing=True,\n","            optim=\"paged_adamw_8bit\",\n","            logging_steps=10,\n","            save_strategy=\"epoch\",\n","            learning_rate=2e-4,\n","            bf16=use_flash_attention2,\n","            fp16=not use_flash_attention2,\n","            tf32=use_flash_attention2,\n","            max_grad_norm=0.3,\n","            warmup_steps=5,\n","            lr_scheduler_type=\"linear\",\n","            disable_tqdm=False,\n","            report_to=\"none\"\n","        )\n","        self.model = get_peft_model(self.model, self.peft_config)\n","\n","    def train_model(self, format_instruction_func):\n","        trainer = SFTTrainer(\n","            model=self.model,\n","            train_dataset=self.train_dataset,\n","            peft_config=self.peft_config,\n","            max_seq_length=2048,\n","            tokenizer=self.tokenizer,\n","            packing=True,\n","            formatting_func=format_instruction_func, \n","            args=self.args,\n","        )\n","        trainer.train()\n","        return trainer"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["'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'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["os.environ[\"HF_TOKEN\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]\n","/workspace/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/workspace/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/workspace/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","Generating train split: 122 examples [00:00, 361.32 examples/s]\n","/workspace/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"]},{"ename":"NameError","evalue":"name '_flash_supports_window_size' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     19\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     20\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     21\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     ]\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     32\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mVariables\u001b[38;5;241m.\u001b[39mFINE_TUNED_MODEL_PATH\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_instruction_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_instruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m trained_model\u001b[38;5;241m.\u001b[39msave_model()\n","Cell \u001b[0;32mIn[3], line 125\u001b[0m, in \u001b[0;36mTrainer.train_model\u001b[0;34m(self, format_instruction_func)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_instruction_func):\n\u001b[1;32m    115\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m    116\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    117\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:451\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 451\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1139\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1136\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:930\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    927\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 930\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    943\u001b[0m         hidden_states,\n\u001b[1;32m    944\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    950\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    951\u001b[0m     )\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:481\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         )\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    484\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    485\u001b[0m     )\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:255\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    252\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 255\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:677\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:500\u001b[0m, in \u001b[0;36mLlamaFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    497\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[1;32m    498\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[0;32m--> 500\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43m_flash_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msliding_window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_top_left_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flash_attn_uses_top_left_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    513\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:180\u001b[0m, in \u001b[0;36m_flash_attention_forward\u001b[0;34m(query_states, key_states, value_states, attention_mask, query_length, is_causal, dropout, softmax_scale, sliding_window, use_top_left_mask, softcap, deterministic)\u001b[0m\n\u001b[1;32m    176\u001b[0m     causal \u001b[38;5;241m=\u001b[39m is_causal \u001b[38;5;129;01mand\u001b[39;00m query_length \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Assuming 4D tensors, key_states.shape[1] is the key/value sequence length (source length).\u001b[39;00m\n\u001b[1;32m    179\u001b[0m use_sliding_windows \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 180\u001b[0m     \u001b[43m_flash_supports_window_size\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m sliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m sliding_window\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    182\u001b[0m flash_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: (sliding_window, sliding_window)} \u001b[38;5;28;01mif\u001b[39;00m use_sliding_windows \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flash_attn_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.4.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mNameError\u001b[0m: name '_flash_supports_window_size' is not defined"]}],"source":["EnvironmentLoader.load_env()\n","dataset_handler = DatasetHandler(data_path=utils.Variables.INSTRUCTION_DATASET_JSON_PATH)\n","train_dataset, test_dataset = dataset_handler.load_and_split_dataset()\n","\n","new_test_dataset = []\n","for dict_ in test_dataset:\n","    dict_['Output'] = ''\n","    new_test_dataset.append(dict_)\n","\n","model_manager = ModelManager(\n","    model_id=\"meta-llama/Meta-Llama-3-8B\",\n","    use_flash_attention2=True,\n","    hf_token=os.environ[\"HF_TOKEN\"]\n",")\n","model, tokenizer = model_manager.load_model_and_tokenizer()\n","model_manager.save_model_and_tokenizer(model, tokenizer, save_directory=utils.Variables.BASE_MODEL_PATH)\n","model = model_manager.prepare_for_training(model)\n","\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n","    ]\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    use_flash_attention2=True,\n","    output_dir=utils.Variables.FINE_TUNED_MODEL_PATH\n",")\n","trained_model = trainer.train_model(format_instruction_func=dataset_handler.format_instruction)\n","trained_model.save_model()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["'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'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["os.getenv('HF_TOKEN')"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","\n","load_dotenv()"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:19.275606Z","iopub.status.busy":"2024-04-21T10:46:19.275240Z","iopub.status.idle":"2024-04-21T10:46:21.696131Z","shell.execute_reply":"2024-04-21T10:46:21.695089Z","shell.execute_reply.started":"2024-04-21T10:46:19.275574Z"},"trusted":true},"outputs":[{"ename":"ValueError","evalue":"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 75\u001b[0m\n\u001b[1;32m     67\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     68\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;28;01mif\u001b[39;00m use_flash_attention2 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_IrWhlJbZgVVHpnXFdrAbCCIOdjpuzXzrxH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_flash_attention2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msdpa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     82\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     86\u001b[0m     model_id,\n\u001b[1;32m     87\u001b[0m     token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# if model is gated like llama or mistral\u001b[39;00m\n\u001b[1;32m     88\u001b[0m )\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 524\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:996\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:772\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 772\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    775\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems()}\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/configuration_llama.py:161\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mlp_bias, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m keys_to_ignore_at_inference \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32000\u001b[39m,\n\u001b[1;32m    146\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    147\u001b[0m     intermediate_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11008\u001b[39m,\n\u001b[1;32m    148\u001b[0m     num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    149\u001b[0m     num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    150\u001b[0m     num_key_value_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     hidden_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m     max_position_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    153\u001b[0m     initializer_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m,\n\u001b[1;32m    154\u001b[0m     rms_norm_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m,\n\u001b[1;32m    155\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     bos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    158\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    159\u001b[0m     pretraining_tp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    160\u001b[0m     tie_word_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m--> 161\u001b[0m     rope_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000.0\u001b[39m,\n\u001b[1;32m    162\u001b[0m     rope_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m     attention_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m     attention_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    165\u001b[0m     mlp_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m ):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m vocab_size\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_position_embeddings \u001b[38;5;241m=\u001b[39m max_position_embeddings\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/configuration_llama.py:182\u001b[0m, in \u001b[0;36m_rope_scaling_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_act \u001b[38;5;241m=\u001b[39m hidden_act\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitializer_range \u001b[38;5;241m=\u001b[39m initializer_range\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrms_norm_eps \u001b[38;5;241m=\u001b[39m rms_norm_eps\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m pretraining_tp\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m use_cache\n","\u001b[0;31mValueError\u001b[0m: `rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","import os\n","from dotenv import load_dotenv\n","import time\n","from random import randrange, sample, seed\n","\n","import torch\n","import os\n","from datasets import load_dataset\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","\n","seed(42)\n","\n","from datasets import load_dataset\n","import utils\n","dataset = load_dataset(\"json\", data_files=utils.Variables.INSTRUCTION_DATASET_JSON_PATH, split=\"train\")\n","\n","def format_instruction(sample):\n","\treturn f\"\"\"    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{sample['Instruction']}\n","\n","### Input:\n","{sample['Input']}\n","\n","### Response:\n","{sample['Output']}\n","\"\"\"\n","\n","from datasets import load_dataset, DatasetDict\n","import utils\n","# Load the entire dataset\n","dataset = load_dataset(\"json\", data_files=utils.Variables.INSTRUCTION_DATASET_JSON_PATH)\n","\n","# Split the dataset into training and testing sets\n","train_test_split = dataset['train'].train_test_split(test_size=0.2)\n","\n","# Create a DatasetDict to hold the splits\n","dataset_dict = DatasetDict({\n","    'train': train_test_split['train'],\n","    'test': train_test_split['test']\n","})\n","\n","# Now you have separate training and testing sets\n","train_dataset = dataset_dict['train']\n","test_dataset = dataset_dict['test']\n","\n","\n","new_test_dataset = []\n","for dict_ in test_dataset:\n","    dict_['Output'] = ''\n","    new_test_dataset.append(dict_)\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Hugging Face model id\n","model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","# model_id = \"mistralai/Mistral-7B-v0.1\"\n","\n","# BitsAndBytesConfig int-4 config \n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id, \n","    quantization_config=bnb_config, \n","    use_cache=False, \n","    device_map=\"auto\",\n","    token=\"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\",#os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n","    attn_implementation=\"flash_attention_2\" if use_flash_attention2 else \"sdpa\"\n",")\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Specify the directory where you want to save the model and tokenizer\n","save_directory = utils.Variables.BASE_MODEL_PATH\n","\n","# Save the model\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(save_directory)\n","\n","\n","# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        r=64,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","            \"q_proj\",\n","            \"k_proj\",\n","            \"v_proj\",\n","            \"o_proj\",\n","            \"gate_proj\", \n","            \"up_proj\", \n","            \"down_proj\",\n","        ]\n",")\n","\n","# Prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","\n","\n","args = TrainingArguments(\n","    output_dir=utils.Variables.FINE_TUNED_MODEL_PATH,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,#6 if use_flash_attention2 else 2, # you can play with the batch size depending on your hardware\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=10,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=use_flash_attention2,\n","    fp16=not use_flash_attention2,\n","    tf32=use_flash_attention2,\n","    max_grad_norm=0.3,\n","    warmup_steps=5,\n","    lr_scheduler_type=\"linear\",\n","    disable_tqdm=False,\n","    report_to=\"none\"\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    max_seq_length=2048,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_instruction, \n","    args=args,\n",")\n","\n","# train\n","trainer.train()\n","\n","# save model\n","trainer.save_model()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /workspace/.venv/lib/python3.12/site-packages (4.43.1)\n","Requirement already satisfied: filelock in /workspace/.venv/lib/python3.12/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (0.24.1)\n","Requirement already satisfied: numpy>=1.17 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /workspace/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /workspace/.venv/lib/python3.12/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /workspace/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /workspace/.venv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /workspace/.venv/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /workspace/.venv/lib/python3.12/site-packages (24.0)\n","Collecting pip\n","  Downloading pip-24.1.2-py3-none-any.whl.metadata (3.6 kB)\n","Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.0\n","    Uninstalling pip-24.0:\n","      Successfully uninstalled pip-24.0\n","Successfully installed pip-24.1.2\n"]}],"source":["!pip install --upgrade pip"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting absl-py==2.1.0 (from -r requirements.txt (line 1))\n","  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting accelerate==0.32.1 (from -r requirements.txt (line 2))\n","  Using cached accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: aiohttp==3.9.5 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.9.5)\n","Requirement already satisfied: aiosignal==1.3.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.3.1)\n","Collecting altair==5.3.0 (from -r requirements.txt (line 5))\n","  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n","Collecting annotated-types==0.7.0 (from -r requirements.txt (line 6))\n","  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n","Collecting anyio==4.4.0 (from -r requirements.txt (line 7))\n","  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: asttokens==2.4.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.4.1)\n","Collecting async-timeout==4.0.3 (from -r requirements.txt (line 9))\n","  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: attrs==23.2.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (23.2.0)\n","Collecting bert-score==0.3.13 (from -r requirements.txt (line 11))\n","  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting bitsandbytes==0.43.1 (from -r requirements.txt (line 12))\n","  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n","Collecting blinker==1.8.2 (from -r requirements.txt (line 13))\n","  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n","Collecting cachetools==5.3.3 (from -r requirements.txt (line 14))\n","  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: certifi==2024.7.4 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2024.7.4)\n","Requirement already satisfied: charset-normalizer==3.3.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (3.3.2)\n","Collecting click==8.1.7 (from -r requirements.txt (line 17))\n","  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: comm==0.2.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.2.2)\n","Collecting contourpy==1.2.1 (from -r requirements.txt (line 19))\n","  Using cached contourpy-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","Collecting cycler==0.12.1 (from -r requirements.txt (line 20))\n","  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: datasets==2.20.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (2.20.0)\n","Requirement already satisfied: debugpy==1.8.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (1.8.2)\n","Requirement already satisfied: decorator==5.1.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (5.1.1)\n","Requirement already satisfied: dill==0.3.8 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (0.3.8)\n","Collecting dnspython==2.6.1 (from -r requirements.txt (line 25))\n","  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: docstring_parser==0.16 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (0.16)\n","Requirement already satisfied: einops==0.8.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.8.0)\n","Collecting email_validator==2.2.0 (from -r requirements.txt (line 28))\n","  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n","Collecting exceptiongroup==1.2.2 (from -r requirements.txt (line 29))\n","  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: executing==2.0.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (2.0.1)\n","Collecting fastapi==0.111.0 (from -r requirements.txt (line 31))\n","  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n","Collecting fastapi-cli==0.0.4 (from -r requirements.txt (line 32))\n","  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n","Collecting filelock==3.15.4 (from -r requirements.txt (line 33))\n","  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n","Collecting flash-attn==2.6.1 (from -r requirements.txt (line 34))\n","  Using cached flash_attn-2.6.1.tar.gz (2.6 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting fonttools==4.53.1 (from -r requirements.txt (line 35))\n","  Using cached fonttools-4.53.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n","Requirement already satisfied: frozenlist==1.4.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (1.4.1)\n","Collecting fsspec==2024.5.0 (from -r requirements.txt (line 37))\n","  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n","Collecting gitdb==4.0.11 (from -r requirements.txt (line 38))\n","  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Collecting GitPython==3.1.43 (from -r requirements.txt (line 39))\n","  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Collecting h11==0.14.0 (from -r requirements.txt (line 40))\n","  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting httpcore==1.0.5 (from -r requirements.txt (line 41))\n","  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting httptools==0.6.1 (from -r requirements.txt (line 42))\n","  Using cached httptools-0.6.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting httpx==0.27.0 (from -r requirements.txt (line 43))\n","  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting huggingface-hub==0.23.4 (from -r requirements.txt (line 44))\n","  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: idna==3.7 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 45)) (3.7)\n","Requirement already satisfied: ipykernel==6.29.5 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (6.29.5)\n","Requirement already satisfied: ipython==8.26.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 47)) (8.26.0)\n","Requirement already satisfied: jedi==0.19.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 48)) (0.19.1)\n","Collecting Jinja2==3.1.4 (from -r requirements.txt (line 49))\n","  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n","Collecting joblib==1.4.2 (from -r requirements.txt (line 50))\n","  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting jsonschema==4.23.0 (from -r requirements.txt (line 51))\n","  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n","Collecting jsonschema-specifications==2023.12.1 (from -r requirements.txt (line 52))\n","  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: jupyter_client==8.6.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 53)) (8.6.2)\n","Requirement already satisfied: jupyter_core==5.7.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 54)) (5.7.2)\n","Collecting kiwisolver==1.4.5 (from -r requirements.txt (line 55))\n","  Using cached kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: markdown-it-py==3.0.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 56)) (3.0.0)\n","Requirement already satisfied: MarkupSafe==2.1.5 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 57)) (2.1.5)\n","Collecting matplotlib==3.9.1 (from -r requirements.txt (line 58))\n","  Using cached matplotlib-3.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: matplotlib-inline==0.1.7 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 59)) (0.1.7)\n","Requirement already satisfied: mdurl==0.1.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 60)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 61)) (1.3.0)\n","Requirement already satisfied: multidict==6.0.5 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 62)) (6.0.5)\n","Requirement already satisfied: multiprocess==0.70.16 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 63)) (0.70.16)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 64)) (1.6.0)\n","Collecting networkx==3.3 (from -r requirements.txt (line 65))\n","  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n","Collecting nltk==3.8.1 (from -r requirements.txt (line 66))\n","  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting numpy==1.26.4 (from -r requirements.txt (line 67))\n","  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from -r requirements.txt (line 68))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from -r requirements.txt (line 69))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from -r requirements.txt (line 70))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from -r requirements.txt (line 71))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from -r requirements.txt (line 72))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from -r requirements.txt (line 73))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from -r requirements.txt (line 74))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from -r requirements.txt (line 75))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from -r requirements.txt (line 76))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from -r requirements.txt (line 77))\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.5.82 (from -r requirements.txt (line 78))\n","  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from -r requirements.txt (line 79))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting orjson==3.10.6 (from -r requirements.txt (line 80))\n","  Using cached orjson-3.10.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","Requirement already satisfied: packaging==24.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 81)) (24.1)\n","Requirement already satisfied: pandas==2.2.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 82)) (2.2.2)\n","Requirement already satisfied: parso==0.8.4 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 83)) (0.8.4)\n","Requirement already satisfied: peft==0.11.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 84)) (0.11.1)\n","Requirement already satisfied: pexpect==4.9.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 85)) (4.9.0)\n","Collecting pillow==10.4.0 (from -r requirements.txt (line 86))\n","  Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n","Requirement already satisfied: platformdirs==4.2.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 87)) (4.2.2)\n","Requirement already satisfied: prompt_toolkit==3.0.47 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 88)) (3.0.47)\n","Collecting protobuf==5.27.2 (from -r requirements.txt (line 89))\n","  Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Requirement already satisfied: psutil==6.0.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 90)) (6.0.0)\n","Requirement already satisfied: ptyprocess==0.7.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 91)) (0.7.0)\n","Collecting pure-eval==0.2.2 (from -r requirements.txt (line 92))\n","  Using cached pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n","Collecting pyarrow==16.1.0 (from -r requirements.txt (line 93))\n","  Using cached pyarrow-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: pyarrow-hotfix==0.6 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 94)) (0.6)\n","Collecting pydantic==2.8.2 (from -r requirements.txt (line 95))\n","  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n","Collecting pydantic_core==2.20.1 (from -r requirements.txt (line 96))\n","  Using cached pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting pydeck==0.9.1 (from -r requirements.txt (line 97))\n","  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: Pygments==2.18.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 98)) (2.18.0)\n","Collecting pyparsing==3.1.2 (from -r requirements.txt (line 99))\n","  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 100)) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv==1.0.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 101)) (1.0.1)\n","Collecting python-multipart==0.0.9 (from -r requirements.txt (line 102))\n","  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pytz==2024.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 103)) (2024.1)\n","Requirement already satisfied: PyYAML==6.0.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 104)) (6.0.1)\n","Requirement already satisfied: pyzmq==26.0.3 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 105)) (26.0.3)\n","Collecting referencing==0.35.1 (from -r requirements.txt (line 106))\n","  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: regex==2024.5.15 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 107)) (2024.5.15)\n","Requirement already satisfied: requests==2.32.3 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 108)) (2.32.3)\n","Requirement already satisfied: rich==13.7.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 109)) (13.7.1)\n","Collecting rouge-score==0.1.2 (from -r requirements.txt (line 110))\n","  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting rpds-py==0.19.0 (from -r requirements.txt (line 111))\n","  Using cached rpds_py-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: safetensors==0.4.3 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 112)) (0.4.3)\n","Collecting shellingham==1.5.4 (from -r requirements.txt (line 113))\n","  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: shtab==1.7.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 114)) (1.7.1)\n","Requirement already satisfied: six==1.16.0 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 115)) (1.16.0)\n","Collecting smmap==5.0.1 (from -r requirements.txt (line 116))\n","  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Collecting sniffio==1.3.1 (from -r requirements.txt (line 117))\n","  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: stack-data==0.6.3 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 118)) (0.6.3)\n","Collecting starlette==0.37.2 (from -r requirements.txt (line 119))\n","  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n","Collecting streamlit==1.36.0 (from -r requirements.txt (line 120))\n","  Using cached streamlit-1.36.0-py2.py3-none-any.whl.metadata (8.5 kB)\n","Collecting sympy==1.13.0 (from -r requirements.txt (line 121))\n","  Using cached sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n","Collecting tenacity==8.5.0 (from -r requirements.txt (line 122))\n","  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: tokenizers==0.19.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 123)) (0.19.1)\n","Collecting toml==0.10.2 (from -r requirements.txt (line 124))\n","  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n","Collecting toolz==0.12.1 (from -r requirements.txt (line 125))\n","  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n","Collecting torch==2.3.1 (from -r requirements.txt (line 126))\n","  Using cached torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n","Requirement already satisfied: tornado==6.4.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 127)) (6.4.1)\n","Requirement already satisfied: tqdm==4.66.4 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 128)) (4.66.4)\n","Requirement already satisfied: traitlets==5.14.3 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 129)) (5.14.3)\n","Collecting transformers==4.42.4 (from -r requirements.txt (line 130))\n","  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n","Collecting triton==2.3.1 (from -r requirements.txt (line 131))\n","  Using cached triton-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: trl==0.9.6 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 132)) (0.9.6)\n","Collecting typer==0.12.3 (from -r requirements.txt (line 133))\n","  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n","Collecting typing_extensions==4.12.2 (from -r requirements.txt (line 134))\n","  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: tyro==0.8.5 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 135)) (0.8.5)\n","Requirement already satisfied: tzdata==2024.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 136)) (2024.1)\n","Collecting ujson==5.10.0 (from -r requirements.txt (line 137))\n","  Using cached ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n","Requirement already satisfied: urllib3==2.2.2 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 138)) (2.2.2)\n","Collecting uvicorn==0.30.1 (from -r requirements.txt (line 139))\n","  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n","Collecting uvloop==0.19.0 (from -r requirements.txt (line 140))\n","  Using cached uvloop-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchdog==4.0.1 (from -r requirements.txt (line 141))\n","  Using cached watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n","Collecting watchfiles==0.22.0 (from -r requirements.txt (line 142))\n","  Using cached watchfiles-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: wcwidth==0.2.13 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 143)) (0.2.13)\n","Collecting websockets==12.0 (from -r requirements.txt (line 144))\n","  Using cached websockets-12.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: xxhash==3.4.1 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 145)) (3.4.1)\n","Requirement already satisfied: yarl==1.9.4 in /workspace/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 146)) (1.9.4)\n","Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n","Using cached accelerate-0.32.1-py3-none-any.whl (314 kB)\n","Using cached altair-5.3.0-py3-none-any.whl (857 kB)\n","Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n","Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n","Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n","Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n","Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n","Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Using cached contourpy-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (309 kB)\n","Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n","Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n","Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n","Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n","Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n","Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n","Using cached fonttools-4.53.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n","Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n","Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n","Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n","Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n","Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n","Using cached httptools-0.6.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n","Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n","Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n","Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n","Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n","Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n","Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n","Using cached kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n","Using cached matplotlib-3.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n","Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n","Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/410.6 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n","\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/410.6 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","from nltk.translate.bleu_score import corpus_bleu\n","from rouge_score import rouge_scorer\n","from transformers import BertTokenizer, BertForMaskedLM, BertModel\n","from bert_score import BERTScorer\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","import time\n","\n","def calculate_bleu_score(machine_results, reference_texts):\n","    bleu_score = corpus_bleu([[ref.split()] for ref in reference_texts], [gen.split() for gen in machine_results])\n","    print(f'BLEU Score: {bleu_score}')\n","\n","def calculate_rouge_scores(generated_answers, ground_truth):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n","    for gen, ref in zip(generated_answers, ground_truth):\n","        scores = scorer.score(gen, ref)\n","        total_rouge1 += scores['rouge1'].fmeasure\n","        total_rouge2 += scores['rouge2'].fmeasure\n","        total_rougeL += scores['rougeL'].fmeasure\n","    average_rouge1 = total_rouge1 / len(generated_answers)\n","    average_rouge2 = total_rouge2 / len(generated_answers)\n","    average_rougeL = total_rougeL / len(generated_answers)\n","    return {'average_rouge1':average_rouge1,\n","            'average_rouge2':average_rouge2,\n","            'average_rougeL':average_rougeL}\n","    print(f'Average ROUGE-1: {average_rouge1}')\n","    print(f'Average ROUGE-2: {average_rouge2}')\n","    print(f'Average ROUGE-L: {average_rougeL}')\n","\n","class ModelHandler:\n","\n","    def __init__(self):\n","        pass\n","\n","    def loading_model(self, model_chosen='fine_tuned_model'):\n","\n","        if model_chosen == 'fine_tuned_model':\n","            model_dir=utils.Variables.FINE_TUNED_MODEL_PATH\n","            self.model = AutoPeftModelForCausalLM.from_pretrained(\n","                model_dir,\n","                low_cpu_mem_usage=True,\n","                torch_dtype=torch.float16,\n","                load_in_4bit=True,\n","                )\n","\n","        elif model_chosen == 'base_model':\n","            model_dir=utils.Variables.BASE_MODEL_PATH\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                model_dir,\n","                low_cpu_mem_usage=True,\n","                torch_dtype=torch.float16,\n","                load_in_4bit=True,\n","                )\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","\n","    def ask_question(self, instruction, temperature=0.5, max_new_tokens = 1000):\n","\n","        prompt = format_instruction(instruction)\n","\n","        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","        start_time = time.time()\n","        with torch.inference_mode():\n","            outputs = self.model.generate(input_ids=input_ids, pad_token_id=self.tokenizer.eos_token_id, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.5,temperature=temperature)\n","        end_time = time.time()\n","\n","        total_time = end_time - start_time\n","        output_length = len(outputs[0])-len(input_ids[0])\n","\n","        self.output = self.tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n","\n","        return self.output\n","\n","    def parse_output(self):    \n","\n","        # Split the text at the word \"Response\"\n","        parts = self.output.split(\"Response:\", 1)\n","\n","        # Check if \"Response\" is in the text and get the part after it\n","        if len(parts) > 1:\n","            response_text = parts[1].strip()\n","        else:\n","            response_text = \"\"\n","\n","        return response_text"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","/workspace/.venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(\n"]},{"data":{"text/plain":["{'average_rouge1': 0.39997816307812206,\n"," 'average_rouge2': 0.2213826792342886,\n"," 'average_rougeL': 0.33508922374837047}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["modelhandler_finetuned = ModelHandler()\n","modelhandler_finetuned.loading_model(model_chosen = 'fine_tuned_model')\n","ground_truths_list = []\n","finetuned_model_generated_answers = []\n","\n","for triple_with_output, triple_without_output in zip(test_dataset, new_test_dataset):\n","\n","    modelhandler_finetuned.ask_question(triple_without_output)\n","    finetuned_model_generated_answer = modelhandler_finetuned.parse_output()\n","    ground_truth = triple_with_output['Output']\n","\n","    finetuned_model_generated_answers.append(finetuned_model_generated_answer)\n","    ground_truths_list.append(ground_truth)\n","\n","    \n","rouge_score_fine_tuned_model = calculate_rouge_scores(finetuned_model_generated_answers,ground_truths_list)\n","rouge_score_fine_tuned_model"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","/workspace/.venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n","Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"name":"stdout","output_type":"stream","text":["Running triple {'Instruction': 'Describe the scope of social bias in NLP models, including LLMs.', 'Input': '\"Various types of social biases in NLP models have been reported, including word vectors, MLMs, and now LLMs.\"', 'Output': '\"Social bias is a widespread issue in NLP models, affecting word vectors, MLMs, and LLMs.\"'}\n","Running triple {'Instruction': 'Describe how adding context to the prompt can improve the reasoning performance of an LLM.', 'Input': '\"In LLMs, adding context to the prompt can increase the ID (depending on how related is the context to the question)\"', 'Output': '\"Adding context to the prompt can increase the intrinsic dimension, leading to more complex input and improved reasoning performance of the model.\"'}\n","Running triple {'Instruction': 'Summarize the goal of model alignment in Large Language Model (LLM) research.', 'Input': '\"The goal of aligning models with human values, known as alignment, is a driving force in current LLM research.\"', 'Output': '\"The goal of model alignment is to align LLMs with human values.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the role of Large Language Models (LLMs) in the Elevate framework.', 'Input': '\"Elevate uses the LLM for constructing the behavior model and exploring the state space to compensate for the loss of semantic information.\"', 'Output': '\"LLMs in Elevate are used to construct behavior models and explore state spaces, compensating for lost semantic information.\"'}\n","Running triple {'Instruction': 'Describe the test sets used for evaluating translation accuracy.', 'Input': '\"We use the test sets employed by Morishita et al. (2022) to evaluate translation accuracy.\"', 'Output': '\"The test sets consist of 13 datasets, including 5 for En⇒Ja, 3 for Ja⇒En, and 5 for both directions, excluding WMT20 and WMT21 data.\"\\n\\n'}\n","Running triple {'Instruction': \"Identify the security measure taken to prevent inversion attacks in ObfuscaTune's architecture.\", 'Input': '\"Note that both the input text and output text are always within the TEE to prevent inversion attacks.\"', 'Output': '\"ObfuscaTune prevents inversion attacks by keeping input and output text within the Trusted Execution Environment (TEE) at all times.\"'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting disease progression in diabetic patients.', 'Input': '\"Age\"', 'Output': '{ \"reasoning\": \"Age is a significant factor in diabetes progression, as older individuals are more likely to develop insulin resistance and experience worsened symptoms. The score is 0.85.\", \"score\": 0.85 }'}\n","Running triple {'Instruction': 'Describe the purpose of reinforcement learning with human feedback (RLHF) in fine-tuning large language models.', 'Input': '\"Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using preference datasets, enabling the LLM to generate outputs that align with human preferences.\"', 'Output': '\"RLHF fine-tunes LLMs to generate outputs that align with human preferences using preference datasets.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the lesson learned from supervised fine-tuning in the Tele-FLM model series.', 'Input': '\"We observe a successful case following the philosophy of “less is more” for fine-tuning.\"', 'Output': '\"Supervised fine-tuning can elicit latent knowledge from pre-trained models with a modest amount of data, but may require more sophisticated techniques and larger volumes of high-quality data for reasoning tasks.\"'}\n","Running triple {'Instruction': 'Summarize the concept of multi-party computation (MPC) in the context of large language models.', 'Input': '\"Crypten: Secure multi-party computation meets machine learning. Advances in Neural Information Processing Systems, 34:4961–4973.\"', 'Output': '\"Multi-party computation (MPC) enables secure and private computation among multiple parties, which can be applied to large language models for secure and trustworthy AI.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of incorporating a Position-wise Feed-Forward Network in the model.', 'Input': '\"While the multi-head attention mechanism can capture local dependencies and semantic information in the input sequence, it may not be sufficient to model complex nonlinear relationships.\"', 'Output': '\"To enhance the model\\'s expressive power and learning capacity by modeling complex nonlinear relationships.\"\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the results of the jailbreak attack experiment on various large language models.', 'Input': '\"As seen in Table 3, our SOS attack demonstrates strong performance across various models and consistently outperforms the GCG method.\"', 'Output': '\"The SOS attack achieved high attack success rates on various LLMs, including Vicuna-7B, Vicuna-13B, Llama 2, and Mistral, outperforming the GCG method.\"'}\n","Running triple {'Instruction': 'Explain the goal of the Traveling Salesman Problem (TSP).', 'Input': '\"The goal of the TSP is to find the shortest route that visits each node exactly once and returns to the starting node.\"', 'Output': '\"The goal is to find the shortest possible route that visits each node exactly once and returns to the starting node.\"'}\n","Running triple {'Instruction': 'Summarize the proof and its implications.', 'Input': '\"Proof. Let α be a scalar such that α ≠ 0. ... This demonstrates that not only is it possible for a single appended token to break the alignment of a model, but given an appropriate set Q, it is possible to attack a model by finding a token with representation such that q ∈ Q is not attended to.\"', 'Output': '\"The proof shows that a single appended token can break the alignment of a model, and it\\'s possible to attack a model by finding a token that is not attended to, given an appropriate set Q.\"'}\n","Running triple {'Instruction': 'Describe the main contribution of the paper \"FedBPT: Efficient Federated Black-Box Prompt Tuning for Large Language Models\".', 'Input': '\"Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, and Holger R Roth. 2023. Fedbpt: Efficient federated black-box prompt tuning for large language models. arXiv preprint arXiv:2310.01467.\"', 'Output': '\"The paper proposes FedBPT, an efficient federated black-box prompt tuning method for large language models, which enables personalized and efficient tuning of language models in a federated learning setting.\"'}\n","Running triple {'Instruction': 'Explain the benefits of using DCoT over CoT.', 'Input': 'Section 4.2: DCoT is Beneficial on Unseen Tasks.', 'Output': 'DCoT remains beneficial on unseen tasks, outperforming CoT on most datasets, and showing consistent and large gains by increasing k on LLM Method AQuA CSQA ObjCnt SV AMP.\\n\\n'}\n","Running triple {'Instruction': 'Summarize the title of the paper by Jindong Wang et al.', 'Input': '\"On the robustness of chatgpt: An adversarial and out-of-distribution perspective.\"', 'Output': '\"The paper explores the robustness of ChatGPT from an adversarial and out-of-distribution perspective.\"\\n\\n**'}\n","Running triple {'Instruction': 'Describe the main contribution of the paper by R. Gurnani et al. on polymer informatics at scale with multitask graph neural networks.', 'Input': '\"R. Gurnani, C. Kuenneth, A. Toland, R. Ramprasad, Polymer informatics at scale with multitask graph neural networks, Chemistry of Materials 35 (4) (2023) 1560–1567.\"', 'Output': '\"The paper introduces a multitask graph neural network approach for polymer informatics, enabling the prediction of multiple polymer properties at scale.\"\\n\\n**'}\n","Running triple {'Instruction': \"Summarize the effect of using different separators on the attack's strength.\", 'Input': '\"We modify how s, ri, and x are separated within T for Vicuna-7B and its default template. By default, s, ri, and x appear on one line separated by single spaces. We run experiments modifying the separator to (i) two spaces and (ii) the newline character.\"', 'Output': '\"Using different separators such as two spaces or newline characters does not increase the attack\\'s strength, but using the newline character reduces the rate of harmful generations.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the concept of cultural alignment in large language models.', 'Input': '\"Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, and Miguel Rodrigues. 2023. Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede’s Cultural Dimensions. Publisher: arXiv Version Number: 1.\"', 'Output': '\"Cultural alignment in large language models refers to the ability of models to understand and adapt to different cultural dimensions, ensuring that their responses are culturally sensitive and appropriate.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the concept of \"receptive field analysis\" in the context of transformer length extrapolation.', 'Input': '\"Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 13522–13537, 2023.\"', 'Output': '\"Receptive field analysis is a method used to understand transformer length extrapolation, which helps analyze how transformers process input sequences of varying lengths.\"\\n\\n**'}\n","Running triple {'Instruction': 'Evaluate the function f(f(1)) given the definition of f(x) = 1 / (x + 2).', 'Input': '\"f(x) = 1 / (x + 2) and f(f(1)) = ?\"', 'Output': '\"f(f(1)) = 3/7\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the purpose of tool learning in large language models.', 'Input': '\"Tool learning aims to augment LLMs with external tools, teaching them how to select appropriate tools, generate correct parameters and ultimately parse execution results to produce correct responses.\"', 'Output': '\"Tool learning enables large language models to use external tools to assist users in completing practical tasks, such as planning itineraries and controlling physical robots.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the reasoning behind assigning a feature importance score to \"Number of times pregnant\" for predicting diabetes.', 'Input': '\"The number of times a patient has been pregnant can be an indicator of their overall health and lifestyle, which can impact their likelihood of developing diabetes.\"', 'Output': '\"The score is based on the logical reasoning that the number of times pregnant is an indicator of overall health and lifestyle, which affects the likelihood of developing diabetes.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the relationship between the scaling of LLMs and their tool-learning abilities.', 'Input': '\"We also observe the scaling law in tool learning where the performance of LLMs, including stability and effectiveness, increases along with the scaling up of their parameters.\"', 'Output': '\"The scaling up of LLMs\\' parameters correlates with their tool-learning abilities, leading to improved performance, stability, and effectiveness.\"'}\n","Running triple {'Instruction': 'Explain the motivation behind searching for single-character tokens that commonly appear as infixes to other sub-tokens in the pre-training data.', 'Input': 'This is motivated by the observation that common tokens such as space will often be merged into other tokens, which may make their appearance as individual tokens more context-specific.', 'Output': 'The motivation is to identify tokens that are often merged with other tokens, making their individual appearance more context-dependent, and thus potentially effective in attacking the model.\\n\\n'}\n","Running triple {'Instruction': 'Identify the type of computer science paper based on its abstract.', 'Input': '\"Foundation matters... The foundation of the database field is, of course, the relational model. ... In my opinion, nothing could be further from the truth! In this talk, I want to sketch the results of some of my own investigations into database foundations over the past twenty years or so; my aim is to convey some of the excitement and abiding interest that is still to be found in those investigations, with a view--I hope--to inspiring others in the field to become involved in such activities.\"', 'Output': 'Database\\n\\n'}\n","Running triple {'Instruction': 'Solve the Traveling Salesman Problem (TSP) for the given graph.', 'Input': '\"Find the shortest possible route that visits each node exactly once and returns to the starting node.\"', 'Output': '{\"length\": 17, \"path\": [0, 2, 1, 3, 5, 4, 7, 6, 0]}\\n\\n'}\n","Running triple {'Instruction': 'Explain the focus of the research by Maharana et al. in one sentence.', 'Input': '\"Improving generation and evaluation of visual stories via semantic consistency.\"', 'Output': '\"The paper aims to improve the generation and evaluation of visual stories by focusing on semantic consistency.\"'}\n","Running triple {'Instruction': 'Summarize the concept of topicwise support in the context of LLMs.', 'Input': '\"One convenient way to present the values of LLMs is to aggregate their responses along particular topics and report the average degree of support.\"', 'Output': '\"Topicwise support measures the average degree of support or opposition to a topic based on a model\\'s responses to related questions.\"'}\n","Running triple {'Instruction': 'Explain how LLM-ICLs can automate code-change-related tasks suffering from data scarcity.', 'Input': '\"LLM-ICLs open up the opportunities to automate the code-change-related tasks suffering from data scarcity. Data scarcity refers to situations where data acquisition is difficult or the amount of data is small.\"', 'Output': '\"LLM-ICLs can automate code-change-related tasks with limited data by leveraging their emergent abilities, making it possible to automate tasks with scarce data.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of the brushing interaction in the projection plane.', 'Input': '\"To enable the brushing mode, users simply need to click the “Brush” button. After brushing, a rectangle will be displayed to indicate the region that the user has brushed in the projection plane.\"', 'Output': '\"The brushing interaction allows users to select a specific region of interest in the projection plane, providing more detailed information about the brushed instances.\"\\n\\n'}\n","Running triple {'Instruction': 'Define the output of the SASRec Adapted model in the given framework.', 'Input': '\"Qu=\\uf8ee \\uf8ef\\uf8ef\\uf8f0qu 1 qu 2 ... qu n\\uf8f9 \\uf8fa\\uf8fa\\uf8fb=SASRec Adapted (Eu), where Qu∈Rn×d denotes the feature matrix of the interaction sequence for user u.\"', 'Output': '\"The SASRec Adapted model outputs the feature matrix Qu of the interaction sequence for user u, which is a matrix of size n×d.\"'}\n","Running triple {'Instruction': 'Explain how participants verified the AI-generated code in the study.', 'Input': '\"In all systems, participants relied on reading and analyzing the AI-generated code and inspecting the intermediary variables for verification. The Stepwise system’s approach of breaking down tasks into smaller steps, along with the side conversation feature available in both Stepwise and Phasewise systems, improved participant confidence in verification.\"', 'Output': '\"Participants verified AI-generated code by reading it line-by-line, inspecting intermediary variables, and asking questions about unfamiliar code. The Stepwise system\\'s step-by-step approach and side conversation feature improved their confidence in verification.\"\\n\\n'}\n","Running triple {'Instruction': 'List some of the papers referenced in the text that are related to language models.', 'Input': '\"Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam. 2020. Energy-based models for text. ... Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\"', 'Output': '\"Some of the referenced papers related to language models include \\'Energy-based models for text\\' by Bakhtin et al. and \\'Language models are few-shot learners\\' by Brown et al.\"'}\n","Running triple {'Instruction': 'Explain the modification made to the GA loss function to enhance stability.', 'Input': 'The GA loss function is modified by incorporating a threshold σ: L′= max(0 ,LGA−σ).', 'Output': 'The modification adds a threshold σ to the GA loss function to improve stability, similar to the approach by Lu et al. (2024).\\n\\n'}\n","Running triple {'Instruction': 'Explain how the number of heads and context length affect the intrinsic dimension of the MLP input in an LLM.', 'Input': '\"We observe that both context length and number of heads are inducing an increase in the number of regions spanned by the MLP in the input space...\"', 'Output': '\"The number of heads and context length increase the intrinsic dimension of the MLP input, enabling it to generate dense partitions.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the benefits of using knowledge graphs in identifying LLM training data.', 'Input': '\"Knowledge graphs model relationships in a transparent way, allowing for the consideration of broad relationships between ideas and content structure.\"', 'Output': '\"Knowledge graphs provide a transparent approach to identifying LLM training data, going beyond simple copying or paraphrasing to consider the organization and presentation of ideas.\"'}\n","Running triple {'Instruction': 'Describe the chat template used for the Vicuna model.', 'Input': '\"A chat between a curious user and an artificial intelligence assistant.\"', 'Output': '\"The assistant gives helpful, detailed, and polite answers to the user’s questions.\"'}\n","Running triple {'Instruction': 'Explain the significance of studying health misinformation on social media.', 'Input': '\"With the rise in LLM-based chatbots and the ever-increasing digitization of social interactions, the study of public interaction and rapid dissemination of health misinformation has increased in urgency and scope.\"', 'Output': '\"The study of health misinformation on social media is crucial due to the growing use of LLM-based chatbots and the rapid spread of misinformation, which can impact public behavior and health outcomes.\"'}\n","Running triple {'Instruction': 'Explain the difference in performance between the W8 and W4 models on the Belebele task.', 'Input': '\"W8 -2.0% 0.7% 0.1% 0.2% -0.1% -0.3% -1.2% 0.7% 0.0% -0.3% 1.3% -1.5% 0.5% -0.1% -1.3% -1.1% -2.3% -0.8% -2.8% -0.4% -1.7% -1.1% -0.6% -0.6% -0.7% ... W4 -6.5% -4.3% -5.0% -7.4% -1.7% -4.9% -5.7% -4.6% -7.0% -5.0% -4.5% -5.6% -8.0% -6.0% -7.0% -4.4% -5.8% -7.8% -7.9% -5.9% -8.8% -3.0% -5.8% -6.0% -5.4% -6.3%\"', 'Output': '\"The W8 model generally outperforms the W4 model on the Belebele task, with an average improvement of around 4-5% across languages.\"\\n\\n'}\n","Running triple {'Instruction': \"Explain the effect of increasing the maximum inference step on a Large Language Model's (LLM) task-solving ability.\", 'Input': '\"The LLM fails to solve a complex task within 6 steps, but completes the task when the maximum inference step is increased to 12.\"', 'Output': '\"Increasing the maximum inference step allows the LLM to benefit from more trials and exploration, leading to task completion.\"\\n\\n'}\n","Running triple {'Instruction': 'Compare the performance of LLM-PEFTs and small pre-trained models on code-change-related tasks.', 'Input': '\"We compare the performance of LLMs to that of small pre-trained models, i.e., CodeT5 and CCT5, on the selected code-change-related tasks.\"', 'Output': '\"LLM-PEFTs outperform LLM-ICLs and are comparable to CCT5, despite having fewer updated parameters, on code-change-related tasks.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the main idea of the paper in one sentence.', 'Input': '\"Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks\"', 'Output': '\"The paper proposes a novel approach to defend against jailbreak attacks in Large Language Models by directly unlearning harmful knowledge.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the relationship between the output of the multi-head attention layer and the intrinsic dimension driven by the sum of the dimensions induced in each individual attention layer.', 'Input': '\"Theorem 2.1 (causal multi-head Minkowski sum)...The ith row of the MHA mapping output lives in the Minkowski sum of single-head convex hulls...\"', 'Output': '\"The output of the multi-head attention layer is related to the intrinsic dimension, which is driven by the sum of the dimensions induced in each individual attention layer.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the concept of a Roleplay Attack in the context of jailbreak attacks on language models.', 'Input': '\"Roleplay Attack 4Require the model to play a single bad role or multiple roles (usually a good role and a bad role) and generate harmful contents.\"', 'Output': '\"A Roleplay Attack involves requiring the model to play a bad role, generating harmful content, often alongside a good role.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting employment status based on the given variable.', 'Input': 'Variable: Health Insurance Plan', 'Output': '{ \"reasoning\": \"Having a health insurance plan can be an indicator of employment status, as many employers offer health insurance as a benefit to their employees. However, this is not always the case, as individuals may have health insurance through other means, such as a spouse\\'s employer or a private plan. Therefore, while there is some correlation between health insurance plan and employment status, it is not a strong predictor. Thus, the score is 0.4.\", \"score\": 0.4 }\\n\\n**'}\n","Running triple {'Instruction': \"Extract the state from the app's output using the in-context learning strategy.\", 'Input': '\"Input: <app\\'s output>, <model. Q>\"', 'Output': '\"<state>\"\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the paper \"Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models\" by C. Lemieux et al.', 'Input': '\"Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models\"', 'Output': '\"The paper presents Codamosa, a method that uses pre-trained large language models to escape coverage plateaus in test generation.\"\\n\\n**'}\n","Running triple {'Instruction': \"Determine the duration of René Heitmann's head coach position at Boldklubben Frem.\", 'Input': '\"René HeitmannDenmark17 July 201027 July 2010Never coached the team …\"', 'Output': '\"10 days\"\\n\\n**'}\n","Running triple {'Instruction': 'Describe the process of generating NL-FL aligned data in the TheoremLlama framework.', 'Input': '\"We deformalize Mathlib4 using a powerful LLM with retrieved examples from a fine-tuned T5 encoder. Subsequently, we bootstrap the NL-FL aligned data by integrating the natural language proofs into Lean4 code via comments.\"', 'Output': '\"The process involves deformalizing Mathlib4 using an LLM and a fine-tuned T5 encoder, and then integrating natural language proofs into Lean4 code via comments to create NL-FL aligned data.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the limitation of using a manual threshold in the GA method.', 'Input': '\"Applying a manual threshold to GA to control the probability range does not guarantee training stability.\"', 'Output': '\"Using a manual threshold in the GA method can be challenging as it may lead to unstable training, and even a small change in the threshold can significantly impact the probability of harmful responses.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the advantage of tuning LLMs with LoRA on code-change-related tasks.', 'Input': '\"The best-performing LLM-PEFTs outperform the best-performing LLM-ICLs across all tasks, with particularly significant differences on CMG and JITCU.\"', 'Output': '\"Tuning LLMs with LoRA significantly improves their performance on code-change-related tasks.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide an example of a title generation task.', 'Input': '\"Table A2\"', 'Output': '\"task1540 parsed pdfs summarization\"'}\n","Running triple {'Instruction': 'Describe the effect of the Doppler effect on the observed frequency and wavelength of sound waves emitted from a moving source.', 'Input': '\"The Doppler effect affects the observed frequency of sound waves emitted from a moving source...\"', 'Output': '\"The observed frequency increases and wavelength decreases when the source is moving towards the observer, and the observed frequency decreases and wavelength increases when the source is moving away from the observer.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe why some models are not affected by the attack of appending a space token.', 'Input': '\"Without space appended, Llama-2 generates a space as the first output for all inputs. However, when a space is appended, its predictions shift to favor alphabetical characters.\"', 'Output': '\"Llama-2 is not affected because it always outputs a space as the initial token, regardless of whether a space is appended, due to its fine-tuning process.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the importance of prompt variation in evaluating task performance and social bias in Large Language Models (LLMs).', 'Input': '\"studies have highlighted that LLMs have the sensitivity to task instruction and prompt, and verification with multiple prompts is crucial in task performance evaluation of LLMs\"', 'Output': '\"Prompt variation is crucial in evaluating task performance and social bias in LLMs due to their sensitivity to task instruction and prompt.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the evaluation setup used for language identification.', 'Input': '\"We report line-level pass rate (LPR), i.e., the percentage of responses for which all lines in the response are in the user’s desired language.\"', 'Output': '\"The evaluation setup for language identification uses line-level pass rate (LPR) to measure the percentage of responses in the user\\'s desired language.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the limitations of using phone logs and screenshots to analyze phone usage behaviors.', 'Input': '\"Phone logs of system data (e.g., screen events and app states) are limited in capturing the complexity of smartphone use. ... screenshots can partly reveal users’ actions, but they are not continuous, and phone use information could be missed during the time gaps.\"', 'Output': '\"Phone logs and screenshots have limitations in capturing phone usage behaviors, as they lack continuity and may miss important details.\"\\n\\n'}\n","Running triple {'Instruction': 'Define the attention map in a transformer layer of an LLM and explain its significance.', 'Input': '\"The attention map is defined as Attn(ℓ) h(X) = softmax causal [XQ(ℓ) hK(ℓ) h⊤X⊤].\"', 'Output': '\"The attention map is a right stochastic matrix that defines a graph where nodes are tokens and edges are attention values, representing the level of connectivity between tokens in the input sequence.\"'}\n","Running triple {'Instruction': 'Summarize the results of linear output head tuning on the OpenAIModEval dataset for the TinyLlama model with a batch size of 8.', 'Input': '\"TinyLlama 8.82 (.01) .79 (.07) .45 (.09) .58 (.06) 3 .06×106\"', 'Output': '\"The TinyLlama model with a batch size of 8 achieved an AUPRC of 0.82, precision of 0.79, recall of 0.45, F1 score of 0.58, and a guard overhead of 3 .06×106.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the role of the model in the conversation view.', 'Input': 'Model Name: gpt-4, Language: English, Category: harassment, sexual, self-harm, violence, hate', 'Output': 'The model (in this case, gpt-4) engages in conversations in the specified language (English) and is categorized based on the content of the conversation, such as harassment, sexual, self-harm, violence, or hate.'}\n","Running triple {'Instruction': 'Provide a brief summary of the search result for rental properties in Santa Monica.', 'Input': '{\"location\": \"Santa Monica\", \"response\": \"{\\'count\\': 50, \\'results\\': [{\\'advertisers\\': [{\\'email\\': None, \\'fulfillment_id\\': None, \\'href\\': None,…\"}', 'Output': '\"50 rental properties found in Santa Monica with available agent contact information.\"\\n\\n**'}\n","Running triple {'Instruction': 'Explain the approach of GPTZero in detecting machine-generated text.', 'Input': '\"proprietary frameworks like GPTZero, devised by Princeton University, focus on indicators such as perplexity and burstiness to analyze texts for machine-generated content identification.\"', 'Output': '\"GPTZero detects machine-generated text by analyzing indicators like perplexity and burstiness.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the concept of feature learning in the presence of spurious correlations, as discussed in the paper by Pavel Izmailov et al.', 'Input': '\"On Feature Learning in the Presence of Spurious Correlations. In Advances in Neural Information Processing Systems, 2022.\"', 'Output': '\"Feature learning in the presence of spurious correlations refers to the challenge of learning meaningful features from data when there are correlations between features that are not causally related.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the benefits of using LLMs in model-based testing.', 'Input': '\"LLMs have been proved with strong natural language processing capabilities and can be directly applied to downstream tasks.\"', 'Output': '\"LLMs can improve the efficiency and accuracy of model-based testing by leveraging their NLP capabilities and ability to learn from small datasets.\"'}\n","Running triple {'Instruction': 'Interpret the results of the Z-score normalization in the context of significance testing.', 'Input': '\"We calculate the associated p-value of the resulting Z-score and report the negative log(p) for illustration purposes in the Fig. 2(B). Values larger than 4.60 are significantly different considering a conservative critical p-value of 0.01.\"', 'Output': '\"The Z-score normalization is used to determine the significance of the results, with values larger than 4.60 indicating a significant difference from random performance at a conservative critical p-value of 0.01.\"'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting the number of points per game of an NBA basketball player.', 'Input': '\"Steals per game\"', 'Output': '{ \"reasoning\": \"Steals per game is a moderately important feature in predicting points per game, as it indicates a player\\'s ability to create scoring opportunities. Thus, the score is 0.6.\", \"score\": 0.6 }\\n\\n'}\n","Running triple {'Instruction': 'Paraphrase a question about a topic.', 'Input': '\"Do you think individuals should have the right to decide how they die?\"', 'Output': '\"In your view, is euthanasia morally acceptable?\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the difference between \"Grey\" and \"Gray\" in a simple and clear manner.', 'Input': 'What is the difference between Grey and Gray? I’ve been told both ways are acceptable, but I’m curious why the vowel switch. Explain like I’m five.', 'Output': 'In the United States, the preferred spelling is \"gray\", which is a color associated with neutrality or lack of enthusiasm. In Britain and some other parts, \"grey\" is used.'}\n","Running triple {'Instruction': 'Describe the purpose of the InternVL model.', 'Input': '\"InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2023.\"', 'Output': '\"The InternVL model aims to scale up vision foundation models and align them for generic visual-linguistic tasks, enabling better performance and flexibility.\"'}\n","Running triple {'Instruction': 'Compare the consistency of base models and fine-tuned models.', 'Input': '\"Base models are more consistent than alignment fine-tuned models, with the exception of llama3 on paraphrase consistency...\"', 'Output': '\"Base models are generally more consistent than fine-tuned models, with some exceptions, such as llama3 on paraphrase consistency.\"'}\n","Running triple {'Instruction': 'Describe the purpose of the Filter Configuration Panel in the Conversation View.', 'Input': 'Fig. 6. Users can create a filter to extract conversations from conversational datasets by writing customized Python code in the Filter Configuration Panel.', 'Output': 'The Filter Configuration Panel allows users to create custom filters to extract specific conversations from datasets using Python code.\\n\\n'}\n","Running triple {'Instruction': 'Describe the evaluation process of JailbreakHunter.', 'Input': '\"We demonstrate how JailbreakHunter assists users in identifying jailbreak prompts from large-scale human-LLM conversational datasets through two case studies and interviews with nine domain experts.\"', 'Output': '\"JailbreakHunter is evaluated through case studies and expert interviews, where domain experts use the system to identify jailbreak prompts from large-scale human-LLM conversational datasets.\"'}\n","Running triple {'Instruction': 'Explain the role of the transformer layer in the hierarchical video aligner.', 'Input': '\"During each aggregation, we fuse the video feature and question feature through a transformer layer, where the video feature is regarded as query while question feature serves as the key and value.\"', 'Output': '\"The transformer layer fuses video and question features to capture their interactions and generate a better alignment.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the advantage of LoRA-Guard over existing approaches in terms of parameter overhead and accuracy.', 'Input': '\"We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.\"', 'Output': '\"LoRA-Guard achieves better performance than existing methods with significantly lower parameter overhead, making it suitable for on-device content moderation.\"'}\n","Running triple {'Instruction': 'Summarize the paper about MMBench-Video, a long-form multi-shot benchmark for holistic video understanding.', 'Input': '\"Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. MMBench-Video: A long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024.\"', 'Output': '\"MMBench-Video is a benchmark for evaluating video understanding models on long-form, multi-shot videos.\"\\n\\n'}\n","Running triple {'Instruction': 'What is the significance of the number 24 in the context of the provided text?', 'Input': '\"24\"', 'Output': '\"The significance of the number 24 is not explicitly stated in the provided text, but it may refer to a specific experiment, result, or parameter in the context of constructing C ACTUS.\"'}\n","Running triple {'Instruction': 'Identify the specific thought that bothers the client the most.', 'Input': '\"I think that everyone sees me as awkward or nerdy. It seems like they all have this ease about them that I just can’t achieve. I end up feeling like I’m just a useless social person.\"', 'Output': '\"The thought that bothers the client the most is that they are a useless social person because people see them as a nerd.\"\\n\\n'}\n","Running triple {'Instruction': 'Identify the dataset with the highest number of optimal k parameters across different models.', 'Input': 'Table 12: Best number of cots (k parameter) for each model and dataset in our best DCoT models according to the dev set.', 'Output': 'Quartz, with a k parameter of 4 for all models.\\n\\n'}\n","Running triple {'Instruction': 'Identify the task type and provide a brief description.', 'Input': '\"task295 semeval 2020 task4 commonsense reasoning\"', 'Output': '\"Explanation: This task involves generating explanations using commonsense reasoning.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the limitations of LoRA-Guard in adapting to different taxonomies.', 'Input': '\"Second, the taxonomy is fixed in our system, and adaptation to different taxonomies requires retraining unlike Llama-Guard which can adapt via in-context learning.\"', 'Output': '\"LoRA-Guard has a fixed taxonomy and requires retraining to adapt to different taxonomies, unlike Llama-Guard which can adapt through in-context learning.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of the KBENCH subset for each Large Language Model (LLM).', 'Input': '\"For each LLM, the final subset of bugs from the KBENCH depends on the chosen retrieval method and the maximum allowed context length.\"', 'Output': '\"The KBENCH subset is used to evaluate the performance of each LLM based on the chosen retrieval method and context length.\"\\n\\n**'}\n","Running triple {'Instruction': 'Describe the main idea of the paper \"Can LLMs express their uncertainty? An empirical evaluation of confidence elicitation in LLMs\" by Miao Xiong et al.', 'Input': '\"The paper investigates whether large language models can express their uncertainty through confidence elicitation.\"', 'Output': '\"The paper explores the ability of large language models to convey their uncertainty through confidence scores, evaluating their performance in uncertainty estimation.\"'}\n","Running triple {'Instruction': 'Summarize the datasets and evaluation metrics used in the experiment.', 'Input': '\"We evaluate our proposed method on two widely-used table-based reasoning benchmarks, Wik iTQ and TabFact. For the table-based fact verification task, we adopt the TabFact dataset, which contains various statements based on Wikipedia tables. We evaluate the dataset using binary classification accuracy on the small-test set containing 1998 statements with 298 different tables. For the table reasoning task, we adopt WikiTable-Question (WikiTQ), which contains open-domain tables accompanied by complex questions. We use denotation accuracy as our evaluation metric, which evaluates the predicted answers based on the gold ones.\"', 'Output': '\"The experiment uses two datasets, Wik iTQ and TabFact, with evaluation metrics of binary classification accuracy and denotation accuracy for table-based fact verification and table reasoning tasks, respectively.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the purpose of Figure 5 in the context of Vicuna-7B training.', 'Input': '\"Figure 5: The training curves of different methods for Vicuna-7B.\"', 'Output': '\"Figure 5 shows the log probability changes during training for harmless and harmful responses to queries.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the hyperparameters used in the experiment.', 'Input': '\"heads h was 4, the hidden size dk was 384, the number of user preferences m was 5, the learning rate was 0.001, the batch size was 128, and the dropout rate was 0.5.\"', 'Output': '\"The experiment used 4 heads, a hidden size of 384, 5 user preferences, a learning rate of 0.001, a batch size of 128, and a dropout rate of 0.5.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the effect of increasing attention heads and context length on the number of regions in the input space of an LLM.', 'Input': '\"Figure 5: LLM input space regions\"', 'Output': '\"Increasing attention heads and context length increases the number of regions, leading to better approximation properties.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of the SOS Attack Framework.', 'Input': 'Algorithm 1: SOS Attack Framework', 'Output': 'The SOS Attack Framework is a method to optimize adversarial embeddings in a Large Language Model (LLM) to create a victim LLM.\\n\\n'}\n","Running triple {'Instruction': 'Summarize the purpose of the ablation study conducted in the paper.', 'Input': '\"We present the following ablation results on models other than Phi-3-small as anonymizers.\"', 'Output': '\"The ablation study evaluates the performance of different models as anonymizers, excluding Phi-3-small.\"\\n\\n'}\n","Running triple {'Instruction': 'Create a question about religious freedom that involves its potential conflict with anti-discrimination laws.', 'Input': '\"Religious Freedom\"', 'Output': '\"Should religious institutions be exempt from anti-discrimination laws?\"'}\n","Running triple {'Instruction': 'Explain the reason for the performance drop of DCoT when increasing k in some cases.', 'Input': '\"DCoT sometimes exhibits some performance drop when increasing k(e.g., Phi-2@4 on GSM8K).\"', 'Output': '\"The performance drop is attributed to an overthinking effect, where the model generates wrong CoTs that bias the final answer.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the concept of multi-task learning in the context of large language models.', 'Input': '\"Multi-task learning allows a model to learn from multiple related tasks simultaneously, which can lead to better generalization and performance improvements across all tasks.\"', 'Output': '\"Multi-task learning enables large language models to learn from multiple related tasks at once, improving their generalization capabilities and performance across various tasks.\"'}\n","Running triple {'Instruction': \"Summarize the client's main concern in the counseling session.\", 'Input': '\"I feel like everyone sees me as a nerd who has no social skills, which is making things really difficult for me at my new job at Google.\"', 'Output': '\"The client is struggling with feelings of social awkwardness and low self-esteem at their new job.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the jailbreak attack in the context of LLMs.', 'Input': '\"Our approach involves optimizing an embedding to permit illegal or otherwise prohibited tasks.\"', 'Output': '\"A jailbreak attack involves optimizing an embedding to allow LLMs to engage in harmful or prohibited tasks, bypassing safety training.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the evaluation metrics used to assess the utility of anonymized texts.', 'Input': '\"We assess the utility of the anonymized texts using the traditional ROUGE score and the LLM-based utility evaluation with the utility judge template Tutility proposed in (Staab et al., 2024).\"', 'Output': '\"The utility of anonymized texts is evaluated using two metrics: ROUGE score, which measures traditional utility, and LLM-based utility evaluation, which considers meaning, readability, and hallucinations.\"'}\n","Running triple {'Instruction': 'Summarize the results of the pairwise comparison on MT-Bench for the quantized LLMs.', 'Input': '\"Table 1 shows the results of pairwise comparison on MT-Bench for various quantized LLMs. Each quantized LLM is compared with the Baseline (16-bit weight) by GPT-4 for their multi-turn responses to the questions in various categories of MT-Bench.\"', 'Output': '\"The pairwise comparison on MT-Bench shows that QDPO achieves the lowest lose-rate and outperforms other quantization methods, including RTN, AWQ, and KD, in terms of answer quality.\"'}\n","Running triple {'Instruction': 'Explain the difference between Feature code change and Refactor code change.', 'Input': '\"The code changes include modifying the functional logic and renaming code entities.\"', 'Output': '\"Feature code change modifies the functional logic, while Refactor code change renames code entities or updates code based on coding standards.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the recommended platform design interventions to combat mental health misinformation.', 'Input': '\"To combat the spread of MHMisinfo, platforms can restrict access to such content by demoting it from users’ search recommendations or removing it altogether.\"', 'Output': '\"Platforms can use large language models for content moderation, incorporate human moderators, and implement additional methods like community reporting to identify suspected MHMisinfo content, while also empowering susceptible users to access relevant mental health resources.\"'}\n","Running triple {'Instruction': 'Describe the upgrades in vision-language comprehension of InternLM-XComposer-2.5 compared to its previous version.', 'Input': '\"Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue.\"', 'Output': '\"InternLM-XComposer-2.5 has three major upgrades in vision-language comprehension: Ultra-High Resolution Understanding, Fine-Grained Video Understanding, and Multi-Turn Multi-Image Dialogue, compared to its previous version.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of the Comparison View in JailbreakHunter.', 'Input': '\"The Comparison View presents the top N most similar reported jailbreak prompts to the selected query, enabling users to assess the similarity between the currently inspected query and reported jailbreak prompts.\"', 'Output': '\"The Comparison View aids users in conducting turn-level analysis, comparing the currently inspected query with reported jailbreak prompts, and assessing their similarity.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the preprocessing steps performed on the textual data.', 'Input': 'All textual data underwent standard preprocessing steps, including tokenization, lowercasing, and punctuation marks.', 'Output': 'The preprocessing steps included tokenization, lowercasing, and removing punctuation marks, as well as domain-specific preprocessing as necessary.\\n\\n'}\n","Running triple {'Instruction': 'Identify the limitations of the study mentioned in the text.', 'Input': '\"We acknowledge several limitations that may influence the interpretation and generalizability of our findings.\"', 'Output': '\"The limitations include reliance on a single, relatively small dataset, focus on only two LLMs, and inability to obtain multiple sets of ratings due to financial constraints.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the process of the better inputs checker in the state space exploration phase.', 'Input': 'Fig. 6. The workflow of better input checker.', 'Output': 'The better inputs checker evaluates the input by checking if it is in the input event set, and if not, returns a *NO INPUT ERROR*. If the input is valid, it determines if there is a better input event based on invocation frequency and history transitions.\\n\\n'}\n","Running triple {'Instruction': 'Identify the rating given to the answer to the question about the impact of minimum wage laws on employment rates and income inequality in the United States by the Mistral model.', 'Input': '\"Mistral 0 10 2 The answer is based on content from one of the provided abstracts, which is cited and referenced appropriately.\"', 'Output': '\"Mistral gave a rating of 0, 10, and 2, indicating that the answer is well-supported by a provided abstract.\"'}\n","Running triple {'Instruction': 'Describe the idea of using large language models as zero-shot reasoners.', 'Input': '\"KOJIMA, T., G U, S. S., R EID, M., M ATSUO, Y., AND IWASAWA, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199–22213.\"', 'Output': '\"Large language models can be used as zero-shot reasoners, enabling them to perform reasoning tasks without requiring explicit training or fine-tuning.\"'}\n","Running triple {'Instruction': 'Describe the main idea of the paper \"Cogcom: Train large vision-language models diving into details through chain of manipulations\".', 'Input': '\"Qi, J., Ding, M., Wang, W., Bai, Y., Lv, Q., Hong, W., Xu, B., Hou, L., Li, J., Dong, Y., et al.: Cogcom: Train large vision-language models diving into details through chain of manipulations.\"', 'Output': '\"The paper proposes Cogcom, a method to train large vision-language models to dive into details through a chain of manipulations, enhancing their performance.\"'}\n","Running triple {'Instruction': 'Describe the preprocessing steps applied to the datasets in the large-scale dataset experiments.', 'Input': '\"For all datasets, we standardize all of the numerical features to have zero mean and unit variance, and one-hot encode all categorical features.\"', 'Output': '\"The preprocessing steps include standardizing numerical features to have zero mean and unit variance, and one-hot encoding categorical features.\"\\n\\n'}\n","Running triple {'Instruction': 'Count the total number of prompt templates provided in the figure.', 'Input': '\"T101 to T127: Prompt templates generated from GPT4\"', 'Output': '\"127\"'}\n","Running triple {'Instruction': 'Identify the key factors to consider when diagnosing a scenario with severe abdominal discomfort.', 'Input': '\"To diagnose a scenario with severe abdominal discomfort, it is first necessary to consider the patient\\'s age, lifestyle, and medical background.\"', 'Output': '\"The key factors to consider when diagnosing a scenario with severe abdominal discomfort are the patient\\'s age, lifestyle, and medical background, as these can help identify the underlying cause of the discomfort.\"\\n\\n'}\n","Running triple {'Instruction': 'Report the AUPRC score of LoRA-Guard trained on ToxicChat and evaluated on OpenAI using the TinyLlama model.', 'Input': 'Table 10', 'Output': '0.8\\n\\n'}\n","Running triple {'Instruction': 'Identify the common theme among the questions in the Income Inequality topic.', 'Input': '\"Do you agree that income inequality is a major cause of social problems in the US? Do you think that the government should implement policies to reduce income inequality? Do you believe that the rich should be taxed more to address income inequality?\"', 'Output': '\"The common theme is addressing income inequality in the US through policies and taxation.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the difference between homogeneous and heterogeneous graphs.', 'Input': '\"Graph-structured data primarily includes homogeneous and heterogeneous graphs. Research on homogeneous graphs often focuses on graph-structure-specific issues, such as protein-protein interaction prediction.\"', 'Output': '\"Homogeneous graphs focus on graph-structure-specific issues, whereas heterogeneous graphs contain rich semantic information and attribute information for nodes and edges.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the role of the evaluator LLM in the proposed defense mechanism.', 'Input': '\"Model inputs and/or outputs are classified as safe or unsafe by an evaluator LLM, allowing the detection of unsafe inputs and outputs, including those induced through adversarial attacks.\"', 'Output': '\"The evaluator LLM classifies model inputs and outputs as safe or unsafe, enabling the detection of unsafe inputs and outputs, including those generated through adversarial attacks.\"'}\n","Running triple {'Instruction': 'Describe the Permutation Feature Importance method for calculating feature importance scores.', 'Input': '\"I used a technique called permutation feature importance. I randomly permuted the value of “Number of times pregnant” for each sample in the dataset 1000 times.\"', 'Output': '\"Permutation Feature Importance involves randomly permuting a feature\\'s values, predicting the outcome, and calculating the importance score based on the difference in predicted probabilities.\"\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the comparison between LLM-ICLs and CodeT5 on code-change-related tasks.', 'Input': '\"The best-performing LLM-ICLs can statistically significantly outperform CodeT5 on CRG, the C++, C#, and Python sub-datasets on CMG.\"', 'Output': '\"LLM-ICLs outperform CodeT5 on some code-change-related tasks, but not all.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the situation described in the post.', 'Input': '\"I saw a text from my SO to his friend about leaving me, can\\'t stop freaking out.\"', 'Output': '\"The user discovered a text message from their partner to a friend, where the partner was considering leaving the user, causing the user to become anxious and worried.\"\\n\\n**'}\n","Running triple {'Instruction': 'Explain the concept of paraphrase consistency in the context of large language models.', 'Input': '\"Paraphrase Consistency Differently expressed but semantically equivalent statements have long provided a standard to judge NLP systems.\"', 'Output': '\"Paraphrase consistency refers to the ability of large language models to provide the same answer to semantically equivalent statements, even if they are expressed differently.\"\\n\\n'}\n","Running triple {'Instruction': 'Define the random chance lower bound for a multi-label classification problem.', 'Input': '\"The chance baseline for a multi-label classification problem is the ratio of positive instances to the total number of instances. Our data consists of 1269 rows and imbalanced class ratio with support for each class as Support = 682,398,35,78,49,18,9.\"', 'Output': '\"The random chance lower bound is the ratio of positive instances to the total number of instances, which is calculated as 39.34 in this case.\"'}\n","Running triple {'Instruction': 'Describe a limitation of the current setup in terms of counseling sessions.', 'Input': '\"In actual counseling sessions, it is common for sessions to last around an hour each, with a total of about 10 sessions typically conducted.\"', 'Output': '\"The current setup involves relatively short interactions compared to real-life counseling sessions, which typically last an hour each and consist of 10 sessions.\"\\n\\n'}\n","Running triple {'Instruction': 'Write a commit message according to the given code change.', 'Input': '\"code change before: \\\\n def calculate_area(radius): \\\\n     return radius * radius \\\\n code change after: \\\\n def calculate_area(radius): \\\\n     return 3.14 * radius * radius \\\\n\"', 'Output': '\"Updated calculate_area function to use pi for more accurate results.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the purpose of Curriculum Data Sorting in training Large Language Models for Lean4 proof writing.', 'Input': '\"We propose a training data sorting technique named Curriculum Data Sorting to enable LLMs to learn this unfamiliar task from easy to difficult.\"', 'Output': '\"Curriculum Data Sorting sorts the training data by difficulty level, allowing LLMs to learn to solve trivial problems before tackling complex proofs, stabilizing the loss curve and improving performance.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the impact of increasing the scale of candidate toolsets on the performance of large language models.', 'Input': '\"We expand the scale of the toolset using two sampling strategies for each task: intra-category sampling and cross-category sampling.\"', 'Output': '\"Increasing the scale of candidate toolsets leads to substantial performance degradation in large language models, as they tend to select useless tools and get stuck.\"\\n\\n'}\n","Running triple {'Instruction': \"Generate context-related input events for a given VPA apps' output.\", 'Input': '\"<app\\'s output >: the real-time VPA apps\\' output.\"', 'Output': '\"<inputs >: the set of context-related inputs generated for <app\\'s output >.\"\\n\\n'}\n","Running triple {'Instruction': 'Identify the model with the highest emotional engagement score in the detailed analysis of narrative quality.', 'Input': 'Table 4. Detailed Analysis of Narrative Quality', 'Output': 'The model with the highest emotional engagement score is \"Our Method\" with a score of 8.6.\\n\\n'}\n","Running triple {'Instruction': 'Explain the ethical considerations for the dataset used in this paper.', 'Input': '\"The dataset used for evaluation in this paper is obtained from open data sources and has been manually verified and screened to eliminate any data with ethical risks and sensitive content.\"', 'Output': '\"The dataset used in this paper is sourced from open data sources and has been manually reviewed to remove any sensitive or ethically risky content.\"\\n\\n'}\n","Running triple {'Instruction': 'Compare the advantages and disadvantages of the Bugatti Veyron and the BMW i8.', 'Input': '\"The second image depicts a Bugatti Veyron, an iconic supercar renowned for its power, speed, and engineering prowess. ... The third image shows a BMW i8, a hybrid sports car that combines electric propulsion with a combustion engine.\"', 'Output': '\"The Bugatti Veyron excels in performance, with a top speed over 250 mph and 1,100 horsepower, but is less practical for daily use due to high maintenance costs and limited seating capacity. In contrast, the BMW i8 offers lower emissions, efficient energy usage, and stylish design, but has a relatively short range compared to fully electric cars.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the abstract of the paper.', 'Input': '\"The explainability of recommendation systems is crucial for enhancing user trust and satisfaction. Leveraging large language models (LLMs) offers new opportunities for comprehensive recommendation logic generation. ... By embedding item titles instead of IDs and utilizing multi-head attention mechanisms, our approach aligns the semantic features of user preferences with those of candidate items, ensuring coherent and user-aligned recommendations.\"', 'Output': '\"The paper proposes a strategy to align large language models with online recommendation systems without fine-tuning, improving explainability and user trust.\"\\n\\n**'}\n","Running triple {'Instruction': 'Describe the purpose of Figure 16.', 'Input': '\"Figure 16: The prompt used for client simulation to construct C ACTUS.\"', 'Output': '\"Figure 16 shows the prompt used to simulate a client for constructing C ACTUS.\"\\n\\n'}\n","Running triple {'Instruction': \"Determine if there is a 'cite' relationship between paper 72 and paper 25.\", 'Input': '{\\\\\"(\\'paper\\', \\'cite\\', \\'paper\\')\\\\\": [], ...}', 'Output': '\"No\"'}\n","Running triple {'Instruction': 'Summarize the finding about the performance of LLM-PEFT in generating commit messages.', 'Input': '\"Finding 15: Users rate that compared to other techniques, LLM-PEFT can generate concise and readable commit messages without compromising too much adequacy.\"', 'Output': '\"LLM-PEFT can generate concise and readable commit messages with moderate adequacy.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the performance of the models in the quality analysis of explanations.', 'Input': '\"The score distribution of all samples in Survey 1 and Survey 2 is shown in Figure 7. We can see that our model is generally above other models in the distribution of the seven metrics, and performs best.\"', 'Output': '\"Our model outperforms other models in the quality analysis of explanations, scoring highest in metrics such as detail, relevance, logic, and satisfaction.\"'}\n","Running triple {'Instruction': 'Explain the effect of different decoding temperatures on the performance of Large Language Models.', 'Input': '\"When the decoding temperature is set to 0.2, it becomes stubborn to repeat the same incorrect actions instead of generating new ones. In contrast, when the temperature is increased to 1.4, the LLM can adaptively correct its mistakes in response to error messages and generate new actions.\"', 'Output': '\"Higher decoding temperatures (e.g., 1.4) encourage Large Language Models to generate more diverse actions and correct mistakes, while lower temperatures (e.g., 0.2) lead to repetitive incorrect actions.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the purpose of the Linguistic Inquiry and Word Count (LIWC) analysis program in the context of mental health.', 'Input': '\"We use the Linguistic Inquiry and Word Count (LIWC) analysis program to understand who engages with MHMisinfo videos on mental health and why.\"', 'Output': '\"LIWC is used to analyze language related to mental health conditions, including affect, community, body, religion, class, and gender, to understand engagement with MHMisinfo videos.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the limitations of using proprietary large language models for explainable recommendation tasks.', 'Input': '\"Representative proprietary models, like ChatGPT-4, exhibit superior language generation and commercial application capabilities relative to open-source models, but there is a misalignment between the user preferences inferred through historical sequence prompts and those used by online recommendation systems.\"', 'Output': '\"Proprietary large language models, such as ChatGPT-4, have limitations in explainable recommendation tasks due to misaligned user preferences, making them less effective for practical deployment.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the dataset splitting process for evaluating LoRA-Guard models.', 'Input': '\"We further split the dataset into train, validation, and test portions to evaluate LoRA-Guard.\"', 'Output': '\"The dataset is split into train, validation, and test sets of sizes 1004, 200, and 456, respectively, with a fixed test split across all experiments.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the limitation of Psych8k-LLAMA3 in a counseling setting.', 'Input': '\"Psych8k-LLAMA3 often presents too much information at once, which can overwhelm clients and lead to ineffective interaction between the counselor and client, with sessions sometimes concluding abruptly.\"', 'Output': '\"Psych8k-LLAMA3 can overwhelm clients with too much information, leading to ineffective interactions and abrupt sessions.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the goal of conducting user studies with more participants.', 'Input': '\"We will conduct user studies with more participants to interact with our system, helping us evaluate the design and implementation of the system.\"', 'Output': '\"Conducting user studies with more participants will help evaluate the design and implementation of the daily activity recognition system.\"'}\n","Running triple {'Instruction': 'Describe the purpose of Figure 21.', 'Input': '\"Figure 21: Interface for human evaluation on dataset quality.\"', 'Output': '\"Figure 21 shows an interface used for human evaluation of dataset quality.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide a score and reasoning for the variable \"Systolic Blood Pressure\" in predicting the risk of an ICU patient developing heart failure.', 'Input': '\"Systolic Blood Pressure\"', 'Output': '{ \"reasoning\": \"Systolic blood pressure is a critical indicator of cardiovascular health, and abnormal values can be a sign of underlying heart failure. Elevated systolic blood pressure can increase the workload on the heart, leading to cardiac dysfunction and heart failure. Therefore, the score is 0.8.\", \"score\": 0.8 }\\n\\n'}\n","Running triple {'Instruction': 'Summarize the main results of the experiment comparing different methods for jailbreak defense.', 'Input': '\"The main results are presented in Table 1 and Table 2. We can clearly observe that Safe Unlearning achieves the lowest ASR on both ID and OOD harmful questions.\"', 'Output': '\"Safe Unlearning achieves the best results in jailbreak defense, with the lowest ASR on both in-domain and out-of-domain harmful questions.\"\\n\\n'}\n","Running triple {'Instruction': 'Identify the limitations of the investigation on bias evaluation in Large Language Models (LLMs).', 'Input': '\"Our work has several limitations. First, our investigation requires much prompt variation regarding task prompt formatting, few-shot setting, and debias-prompts.\"', 'Output': '\"The limitations include limited prompt variation, evaluation only on English datasets, focus on gender bias, and use of only QA dataset for bias evaluation.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the dataset used for visual story generation.', 'Input': '\"This dataset comprises image sequences and their corresponding narrative descriptions, sourced from a variety of domains including comics, illustrated books, and educational content.\"', 'Output': '\"The dataset consists of image sequences with narrative descriptions from various domains like comics, books, and educational content.\"\\n\\n**'}\n","Running triple {'Instruction': 'Explain the relative performance of Aya models on FLORES compared to FP16.', 'Input': 'Table A16: Relative performance (%∆) vs. FP16 for Aya Models on FLORES.', 'Output': 'The table shows the relative performance of Aya models on FLORES compared to FP16, with results presented as percentage changes (∆%) for different languages and models (W8, W4, etc.).\\n\\n'}\n","Running triple {'Instruction': 'Explain how the sequence length and number of attention heads affect the geometry of Large Language Models (LLMs).', 'Input': '\"sequence length as well as the number of attention heads affects the geometry of the LLM.\"', 'Output': '\"The geometry of LLMs is affected by sequence length and number of attention heads, which in turn impact their reasoning capabilities.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the concept of curriculum learning in the context of automated theorem proving.', 'Input': '\"Curriculum learning: A survey\" by Petru Soviany et al.', 'Output': '\"Curriculum learning is a method that involves training models on a curriculum of tasks, gradually increasing in difficulty, to improve their performance in automated theorem proving.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the benefits of using QDPO with LoRA for quantized LLMs.', 'Input': '\"QDPO with LoRA significantly enhances the conversational ability of quantized LLMs, achieving levels nearly identical to those of QDPO. Moreover, QDPO with LoRA reduces the required memory by keeping the quantized base weights fixed and significantly decreases the number of training parameters...\"', 'Output': '\"QDPO with LoRA improves conversational ability in quantized LLMs while reducing memory requirements and the number of training parameters, making it a complementary method for enhancing model performance.\"'}\n","Running triple {'Instruction': 'Summarize the research focus of the paper \"Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models\".', 'Input': '\"Chenyang Lyu, Minghao Wu, and Alham Fikri Aji. 2024. Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models. arXiv preprint. ArXiv:2402.13887 [cs].\"', 'Output': '\"The paper explores the misalignment in evaluating large language models, going beyond probabilities.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the role of the LLM-as-a-Judge in evaluating model outputs.', 'Input': '\"We assess quantized model outputs using LLM-as-a-Judge. In the former, an LLM selects a preferred response from a <instruction, modelA_completion, modelB_completion > tuple.\"', 'Output': '\"The LLM-as-a-Judge evaluates model outputs by selecting a preferred response from a tuple of instructions and model completions, simulating human preference.\"'}\n","Running triple {'Instruction': 'Explain the concept of reframing in the context of text style transfer.', 'Input': '\"Reframing is a text style transfer technique that alters the frame, perspective, or focus of an utterance, aiming to highlight different aspects of the content while preserving its original meaning.\"', 'Output': '\"Reframing is a technique that changes the perspective or focus of a sentence while keeping its original meaning, making certain aspects more salient.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting whether a YouTube channel has more than 20 million subscribers.', 'Input': '\"Total number of views\"', 'Output': '{\"reasoning\": \"The total number of views is a strong indicator of a channel\\'s popularity, which is likely to be correlated with the number of subscribers. Thus, the score is 0.8.\", \"score\": 0.8}\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the performance of various large language models in graph understanding and reasoning tasks.', 'Input': '\"Among all tasks, graph understanding tasks are generally less challenging for the models, while graph reasoning tasks are more difficult.\"', 'Output': '\"LLMs perform better in graph understanding tasks than in graph reasoning tasks.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the concept of \"Superpositions of Cultural Perspectives\" in Large Language Models.', 'Input': '\"Large Language Models as Superpositions of Cultural Perspectives\"', 'Output': '\"Large language models can be seen as superpositions of cultural perspectives, capturing diverse cultural nuances and biases, which is essential for developing culturally sensitive AI systems.\"'}\n","Running triple {'Instruction': 'Summarize the impact of query augmentation methods on the performance of the ALTER model.', 'Input': '\"We conducted experiments on the WikiTQ and TabFact datasets by discarding the step-back augmentation module and the sub-query augmentation module.\"', 'Output': '\"Employing both query augmentation methods simultaneously yields the best performance under all experimental settings.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain why unlearning can generalize well to out-of-distribution (OOD) harmful questions.', 'Input': '\"Many OOD harmful expressions unseen during training are effectively unlearned... We attribute this phenomenon to the model’s grouped representation of diverse harmful responses.\"', 'Output': '\"Unlearning can generalize well to OOD harmful questions because the model groups similar harmful responses together, allowing it to unlearn numerous OOD harmful expressions.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the challenges in identifying jailbreak prompts in conversational datasets.', 'Input': '\"The volume of this conversation data is immense, especially considering the large user base, which can easily reach millions in public conversation data alone. Additionally, the methods used for jailbreaking are diverse and constantly evolving.\"', 'Output': '\"Identifying jailbreak prompts is challenging due to the immense volume of conversation data and the diverse, constantly evolving methods used for jailbreaking.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the approach taken to address ethical concerns in the data collection process.', 'Input': '\"We ensured the exclusion of any personally identifiable information, with all data undergoing anonymization before any analysis was conducted.\"', 'Output': '\"The approach taken was to anonymize the data and exclude personally identifiable information to uphold the principles of privacy and confidentiality.\"'}\n","Running triple {'Instruction': 'Describe the challenges in training LLMs for formal theorem proving.', 'Input': '\"Most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language theorem-proving data.\"', 'Output': '\"The lack of aligned natural language and formal language data for theorem proving hinders the training of LLMs, leading to suboptimal performance.\"'}\n","Running triple {'Instruction': 'Compare the performance of two tool-use frameworks, ReAct and DFSDT, in terms of success rate and inference cost.', 'Input': 'Table 5: The results on two datasets with different tool-use frameworks.', 'Output': 'DFSDT significantly outperforms ReAct with an average of 30.76% point improvement in success rate, but at the cost of nearly triple the number of tokens.\\n\\n'}\n","Running triple {'Instruction': 'Describe the results of the manual analysis on the self-correction abilities of DCoT-tuned models.', 'Input': '\"We find that in 45% of the cases, the second CoT of DCoT@2 exhibits a different reasoning chain than the first CoT, indicating self-correction abilities.\"', 'Output': '\"The manual analysis shows that DCoT-tuned models exhibit self-correction abilities, with 45% of cases demonstrating a different reasoning chain in the second CoT compared to the first.\"'}\n","Running triple {'Instruction': 'Summarize the main finding about the steerability of large language models to Schwartz values.', 'Input': '\"We do not find that models are steerable to Schwartz values, as unrelated values are more influential than relevant ones (Fig. 9).\"', 'Output': '\"Large language models are not steerable to Schwartz values, with unrelated values having more influence than relevant ones.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the filtering process used to ensure the quality of the CACTUS dataset.', 'Input': '\"To ensure the quality of CACTUS, we initially filter out dialogues exhibiting abnormal formats or an insufficient number of turns. Subsequently, we utilize the Cognitive Therapy Rating Scale (CTRS) to evaluate general counseling skills and CBT-specific skills.\"', 'Output': '\"The CACTUS dataset is filtered to remove dialogues with abnormal formats or too few turns, and then evaluated using the Cognitive Therapy Rating Scale (CTRS) to ensure high-quality counseling dialogues.\"'}\n","Running triple {'Instruction': 'Describe the hardware setup used for the experiments in this paper.', 'Input': '\"All experiments in this paper were conducted on GPU clusters with 4 NVIDIA A100 GPUs.\"', 'Output': '\"The experiments used a GPU cluster with 4 NVIDIA A100 GPUs.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the main finding of Figure 27.', 'Input': '\"Model logprobs consistently place most weight on the option letter, regardless of inclusion of an abstention option.\"', 'Output': '\"The model prioritizes the option letter over other options, including abstention.\"'}\n","Running triple {'Instruction': 'Describe the purpose of QDPO in quantized RLHF-tuned models.', 'Input': '\"QDPO demonstrates further recovery of conversational ability... This demonstrates that QDPO effectively enhances the conversational ability of a quantized RLHF-tuned model...\"', 'Output': '\"QDPO aims to recover conversational ability in quantized RLHF-tuned models by reducing the lose-rate and restoring the baseline model\\'s performance.\"\\n\\n'}\n","Running triple {'Instruction': \"Describe the counselor's approach to helping the client address their negative thoughts.\", 'Input': '\"Let\\'s start by identifying some specific thoughts you have during these interactions. Can you give me an example of a thought that crosses your mind in those moments? ... We can start with something simple and see how you feel afterward. If it doesn’t go as planned, that’s okay too; it’s all part of the learning process.\"', 'Output': '\"The counselor takes a gentle and exploratory approach, encouraging the client to identify and examine their negative thoughts, and proposing small, manageable steps to test these thoughts in a non-judgmental and supportive environment.\"'}\n","Running triple {'Instruction': 'Explain the importance of teaching a model to respond to harmful queries.', 'Input': '\"As mentioned above, if we only unlearn harmful responses, the model is prone to degeneration that leads to nonsensical and repetitive answers to harmful queries, simply due to the lack of supervision on the reallocated probability mass.\"', 'Output': '\"Teaching a model to respond to harmful queries is crucial to prevent degeneration and nonsensical answers due to lack of supervision.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the method used to measure similarity between distributions in the context of value consistency.', 'Input': '\"Following best practices, we use the symmetric Jensen-Shannon divergence which allows us to compare between distributions (namely, option-token log probabilities) directly.\"', 'Output': '\"The symmetric Jensen-Shannon divergence is used to measure similarity between distributions, allowing for direct comparison of option-token log probabilities.\"'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting the selling price of a used car based on a given feature.', 'Input': '\"Variable: Age\"', 'Output': '{ \"reasoning\": \"The age of a used car is a significant factor in determining its selling price, as newer cars tend to be more valuable than older ones. The age of a car also affects its condition, maintenance costs, and overall appeal to buyers. Thus, the score is 0.9.\", \"score\": 0.9 }\\n\\n**'}\n","Running triple {'Instruction': 'Identify the type of paper with ID 3.', 'Input': '\"{\\'Database\\': [1, 2, 5, 11, 12, 13, 16, 19, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 41, 42, 46, 49, 51, 53, 59, 61, 62, 68, 71, 75, 78, 80, 83, 89], \\'Wireless Communication\\': [3, 4, 6, 15, 27, 36, 55, 57, 60, 63, 64, 67, 70, 72, 77, 79, 81, 82, 84, 86, 91, 92, 94], \\'Data Mining\\': [7, 8, 9, 10, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 33, 40, 43, 44, 45, 47, 48, 50, 52, 54, 56, 58, 65, 66, 69, 73, 74, 76, 85, 87, 88, 90, 93]}\"', 'Output': '\"Wireless Communication\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the KV cache mechanism in the context of large language models.', 'Input': 'Figure 2 illustrates the KV cache mechanism in both static and real-time editing settings for large language models.', 'Output': 'The KV cache stores precomputed Key/Value pairs that the model leverages to generate predictions in the static setting, and updates in real-time editing settings to maintain accurate information.\\n\\n'}\n","Running triple {'Instruction': 'Identify the cognitive distortion in the given thought and explain it.', 'Input': '\"I fell off my skateboard yesterday, I’m a terrible athlete.\"', 'Output': '\"The cognitive distortion in this thought is labeling, which involves defining a person based on one action or characteristic.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the benefits of using large language models in recommendation systems.', 'Input': '\"The advent of large language models (LLMs) with their robust language comprehension and generation capabilities opens new avenues for bolstering the explainability of recommendation systems.\"', 'Output': '\"Large language models can improve the explainability of recommendation systems by leveraging their robust language comprehension and generation capabilities.\"'}\n","Running triple {'Instruction': 'Analyze customer reviews and provide an overall rating for each phone.', 'Input': '\"TechRadar\\'s review of the latest iPhone: ... (rest of the review)\\\\nCNET\\'s review of the latest Samsung Galaxy: ... (rest of the review)\\\\nThe Verge\\'s review of the latest Google Pixel: ... (rest of the review)\"', 'Output': '\"{ \\\\\"iPhone\\\\\": 9.0, \\\\\"Samsung Galaxy\\\\\": 8.5, \\\\\"Google Pixel\\\\\": 9.0 }\"\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the main issue with traditional methods for Large Language Models (LLMs) in Multi-Hop Question Answering (MHQA) tasks.', 'Input': '\"traditional methods where LLMs may produce errors in the intermediate reasoning process but still arrive at the correct answer.\"', 'Output': '\"LLMs may misinterpret intermediate reasoning steps, but still yield correct answers, hinting at underlying data leakage and speculation.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the architecture of the internal state-based estimator for hallucination risk estimation.', 'Input': '\"The estimator is mathematically formulated as: H= down(up(xq)×SiLU(gate(xq))) where SiLU is the activation function. down, up, and gate are linear layers for down-projection, up-projection, and gate mechanisms, respectively.\"', 'Output': '\"The internal state-based estimator uses a variant of the multilayer perceptron (MLP) with a specific mathematical formulation to handle the complexity of hallucination risk estimation.\"'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting whether a low-income individual has coverage from public health insurance.', 'Input': 'Variable: Age', 'Output': '{ \"reasoning\": \"Age is a crucial factor in determining an individual\\'s eligibility for public health insurance programs. For example, individuals below a certain age may be eligible for Medicaid or other programs. Therefore, the score is 0.8.\", \"score\": 0.8 }\\n\\n'}\n","Running triple {'Instruction': 'Describe the NL-FL Bootstrapping method.', 'Input': '\"We propose the novel NL-FL Bootstrapping. This is a simple but effective method to enhance the LLMs’ Lean4 proof writing ability by integrating natural language reasoning into Lean4 proofs in Mathlib4.\"', 'Output': '\"NL-FL Bootstrapping integrates natural language reasoning into Lean4 proofs by documenting natural language proofs to Lean4 code through comments, enhancing LLMs\\' Lean4 proof writing ability.\"\\n\\n'}\n","Running triple {'Instruction': 'Extract the director of the romantic comedy \"Big Stone Gap\" from the given context.', 'Input': '\"Big Stone Gap (film) section\"', 'Output': '[\"Adriana Trigiani\", \"is the director of\", \"Big Stone Gap\"]'}\n","Running triple {'Instruction': 'Explain the concept of multi-modal large language models mentioned in the paper by Rohan Bavishi et al.', 'Input': '\"Introducing our multimodal models, 2023.\"', 'Output': '\"Multimodal large language models are AI systems that can process and generate multiple types of data, such as text, images, and videos, to perform various tasks and answer questions.\"'}\n","Running triple {'Instruction': 'Summarize the paper \"Questeval: Summarization asks for fact-based evaluation\" by Thomas Scialom et al.', 'Input': '\"In this paper, the authors propose a new evaluation method for summarization tasks, focusing on fact-based evaluation.\"', 'Output': '\"The paper introduces Questeval, a fact-based evaluation method for summarization tasks.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the main finding of the experiment comparing the performance of LLM-ICLs with code input format on JITCU.', 'Input': '\"Table 13. [RQ3] - GLEU and ACC scores of LLM-ICLs with the code input format on JITCU.\"', 'Output': '\"Using code as input generally results in worse performance compared to using diff as input, except for the CodeGen family.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the approach of using large language models for generating training data in the paper \"Generating training data with language models: Towards zero-shot language understanding\" by Yu Meng et al.', 'Input': '\"Generating training data with language models: Towards zero-shot language understanding\"', 'Output': '\"The paper proposes using large language models to generate training data, enabling zero-shot language understanding by leveraging the models\\' ability to generate diverse and relevant data.\"'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for the \"Age\" feature in predicting whether an individual\\'s income is greater than $50,000.', 'Input': '\"Age\"', 'Output': '{ \"reasoning\": \"Age can be an important factor in determining income, as older individuals tend to have more work experience and higher earning potential. However, age alone is not a strong predictor, as many other factors such as occupation and education level also play a significant role. Thus, the score is 0.6.\", \"score\": 0.6 }\\n\\n'}\n","Running triple {'Instruction': 'Summarize the previous work on positive reframing.', 'Input': '\"Ziems et al. (2022) constructed a parallel dataset of original sentences reflecting negative, stress-related tweets, along with crowd-sourced, manually crafted novel reframings. With these dataset and psychological strategies, they used transformer-based models and succeeded in performing positive reframings.\"', 'Output': '\"Ziems et al. (2022) created a dataset of negative tweets and reframings, and used transformer-based models to perform positive reframings.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide a feature importance score and reasoning for predicting the total pay given to a rideshare driver from a trip.', 'Input': '\"Trip duration\"', 'Output': '{ \"reasoning\": \"Trip duration is a highly important feature in predicting the total pay given to a rideshare driver, as longer trips typically result in higher earnings. Thus, the score is 0.8.\", \"score\": 0.8 }\\n\\n'}\n","Running triple {'Instruction': 'Explain the concept of Guardrails in the context of large language models.', 'Input': '\"Some Guardrails are based on the self-defence principle whereby an LLM is used to evaluate the safety of user-provided prompts or model-generated responses...\"', 'Output': '\"Guardrails are defense mechanisms that use large language models to evaluate the safety of prompts or responses and detect undesirable content.\"\\n\\n'}\n","Running triple {'Instruction': 'Summarize the paper \"Measuring Massive Multitask Language Understanding\" by Dan Hendrycks et al.', 'Input': '\"Measuring Massive Multitask Language Understanding. International Conference on Learning Representations, 2021.\"', 'Output': '\"The paper presents a method for measuring the language understanding capabilities of large language models across multiple tasks.\"\\n\\n'}\n","Running triple {'Instruction': 'Describe the limitations of using LLMs for evaluating scientific syntheses.', 'Input': '\"Both LLMs’ rationales occasionally contained inaccuracies or flaws, raising concerns about the credibility of their scores. Moreover, the extent to which the responses are evaluated as syntheses and not simply as answers, without reliance on general knowledge, remains unclear.\"', 'Output': '\"LLMs may produce flawed or inaccurate rationales, and their evaluation of scientific syntheses may rely too heavily on general knowledge rather than synthesizing multiple abstracts.\"'}\n","Running triple {'Instruction': 'Summarize the potential of utilizing screen text data for time-killing detection.', 'Input': '\"By combining app sensor data with screen text analysis, we believe that more accurate time-killing detection can be achieved.\"', 'Output': '\"Screen text data can be used to recognize contextual features of time-killing moments, and combining it with app sensor data can lead to more accurate time-killing detection and personalized interventions for unwanted phone usage.\"'}\n","Running triple {'Instruction': 'Explain the concept of chemical language models, as described in the paper by C. Kuenneth et al. on polybert.', 'Input': '\"C. Kuenneth, R. Ramprasad, polybert: a chemical language model to enable fully machine-driven ultrafast polymer informatics, Nature Communications 14 (1) (2023) 4099.\"', 'Output': '\"Chemical language models, such as polybert, are AI models that can process and understand chemical structures and properties, enabling machine-driven polymer informatics and property predictions.\"'}\n","Running triple {'Instruction': 'Explain the difference between internal and external factors affecting the stability of tool-use models.', 'Input': '\"The internal factors indicate uncertainties during the development of tool-use models from the developers’ perspective... Different from the internal factors, external factors primarily involve diverse prompt engineering when interacting with established tool-use models...\"', 'Output': '\"Internal factors refer to uncertainties during model development, such as decoding temperature and foundation LLMs, whereas external factors involve prompt engineering and user interactions, like query styles and toolset changes.\"\\n\\n'}\n","Running triple {'Instruction': 'Provide a safety rule for cyclists mentioned in the generation examples.', 'Input': '\"Wear a helmet: Always wear a helmet when cycling to protect your head from injuries in case of a fall or collision.\"', 'Output': '\"Always wear a helmet while cycling to protect your head from injuries.\"'}\n","Running triple {'Instruction': 'Describe the layout of a website for a research institution.', 'Input': '\"The website is for Shanghai AI lab. The top navigation bar is blue. Below left, an image shows the logo of the lab. In the right, there is a passage of text that describes the mission of the laboratory.\"', 'Output': '\"The website has a blue top navigation bar, a logo of Shanghai AI lab on the left, and a mission statement on the right.\"\\n\\n**'}\n","Running triple {'Instruction': 'Summarize the issue with concatenating suffixes to attack Llama-2.', 'Input': '\"We observe a high variance in ASR when two fully trained suffixes for the generator and evaluator are concatenated (without further training) and used to attack a Llama-2 evaluator.\"', 'Output': '\"Concatenating suffixes without further training leads to unpredictable ASR when attacking Llama-2, due to unexpected interactions between generator and evaluator suffixes.\"'}\n","Running triple {'Instruction': 'Summarize the approach of IncogniText in anonymizing text against private attribute inference.', 'Input': '\"IncogniText anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value.\"', 'Output': '\"IncogniText anonymizes text to prevent private attribute inference by misleading the adversary.\"\\n\\n'}\n","Running triple {'Instruction': 'Compare the performance of different Large Language Models (LLMs) with various prompting techniques.', 'Input': '\"Table 1. Comparison results for four state-of-the-art LLMs with different prompting techniques: zero-shot (ZS), few-shot, chain-of-thought (CoT), few-shot CoT and Self-Consistency(SC) prompting techniques.\"', 'Output': '\"The performance of LLMs varies with different prompting techniques, with GPT-4.0 achieving the highest F1 score of 53.78% using self-consistency prompting, and Mixtral-8x7B showing the best performance with an F1 score of 52.72% using few-shot CoT prompting.\"'}\n","Running triple {'Instruction': 'Summarize the objective of the proposed FedBiscuit algorithm.', 'Input': '\"The objective is to train multiple binary selectors without bias towards a small specific group, ensuring that each selector is trained using evenly disjoint clusters of clients.\"', 'Output': '\"FedBiscuit aims to train multiple binary selectors without bias, using disjoint clusters of clients to ensure fair training.\"'}\n","Running triple {'Instruction': 'Identify the language model with the best performance on the CMG task with diff as input for a specific programming language (e.g., Java).', 'Input': 'Table 20 showing the performance of various language models on the CMG task with diff as input, with a focus on the Java language.', 'Output': 'For the Java language, CodeLlama-7b has the best performance with a score of 6.07.\\n\\n'}\n","Running triple {'Instruction': \"Summarize the Iterative Proof Writing strategy for harnessing the Large Language Model's ability to prove Lean4 theorems.\", 'Input': '\"This method involves initially having the prover finish as many theorems in a dataset as possible, then using the Lean-verified correct proofs as additional examples for proof writing in the next iteration.\"', 'Output': '\"The Iterative Proof Writing strategy involves using the prover to generate correct proofs, then using those proofs as additional examples to improve the prover\\'s ability to write Lean4 proofs in subsequent iterations.\"'}\n","Running triple {'Instruction': \"Describe the strategy used to improve the LLM's capability in the decision-making task of state space exploration.\", 'Input': '\"We employ a strategy combining in-context learning and chain-of-thought. We prompt the LLM to think step-by-step and show its thinking process.\"', 'Output': '\"The strategy used to improve the LLM\\'s capability involves combining in-context learning and chain-of-thought, where the LLM is prompted to think step-by-step and show its thinking process.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain why chatbots are given initial instructions to not output harmful content.', 'Input': '\"Chatbots are generally given initial instructions to not output harmful, misleading, or biased content, to follow instructions, and to generate informative replies.\"', 'Output': '\"Chatbots are given these instructions to ensure they are safe for deployment in sensitive settings, such as mental health, customer service, and healthcare, where direct human interaction is involved.\"\\n\\n'}\n","Running triple {'Instruction': 'Analyze the effect of appending a space to model templates on the predicted token types.', 'Input': '\"How do the predicted token types change when a space is appended to model templates?\"', 'Output': '\"When a space is appended, the predicted token types shift towards numerical tokens, with some models predicting lists rather than refusals.\"'}\n","Running triple {'Instruction': 'Summarize the results of the experiment comparing different models.', 'Input': 'We tested our models on various models on the test set. The results can be viewed in (Table: 3).', 'Output': 'The BiLSTM+RoBERTa model achieved a competitive ranking of 46 out of 125 participants, while the GRU+RoBERTa model achieved a significantly better result with an accuracy increase of approximately 4%.'}\n","Running triple {'Instruction': 'Rank the features according to their importance in predicting the disease progression status in diabetes patients.', 'Input': '\"Given a list of features, rank them according to their importances in predicting the disease progression status in diabetes patients.\"', 'Output': '1. [Feature 1] 2. [Feature 2] 3. [Feature 3] ... (assuming the features are provided)\\n\\n'}\n","Running triple {'Instruction': 'Summarize the main contribution of the LLaMA paper.', 'Input': '\"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. LLaMA: Open and efficient foundation language models.\"', 'Output': '\"The LLaMA paper introduces open and efficient foundation language models.\"\\n\\n'}\n","Running triple {'Instruction': 'Compare the privacy protection of IncogniText with other anonymization methods.', 'Input': '\"Table 1 presents our main results. We find that IncogniText achieves the highest privacy protection, i.e., lowest attacker inference accuracy, with a tremendous improvement of ca. 19% compared to the strongest baseline.\"', 'Output': '\"IncogniText provides the highest privacy protection, outperforming other anonymization methods with a 19% improvement in attacker inference accuracy.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the importance of providing users with necessary controls in AI-assisted data analysis tools.', 'Input': '\"Our study finds that the key to designing AI-assisted data analysis tools lies in providing the user with the necessary controls to make informed decisions and maintain control over the process.\"', 'Output': '\"Providing users with necessary controls is crucial in AI-assisted data analysis tools as it enables them to make informed decisions and maintain control over the process.\"\\n\\n'}\n","Running triple {'Instruction': 'Explain the meaning of the \"Guard Overhead\" column in the tables.', 'Input': 'The \"Guard Overhead\" column in Tables 4, 5, and 6.', 'Output': 'The \"Guard Overhead\" column represents the number of additional parameters needed to run the guard model with respect to the chat model.\\n\\n'}\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m triple_with_output, triple_without_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_dataset, new_test_dataset):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning triple \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriple_with_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodelhandler_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriple_without_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     base_model_generated_answer \u001b[38;5;241m=\u001b[39m modelhandler_base\u001b[38;5;241m.\u001b[39mparse_output()\n\u001b[1;32m     12\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m triple_with_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","Cell \u001b[0;32mIn[4], line 67\u001b[0m, in \u001b[0;36mModelHandler.ask_question\u001b[0;34m(self, instruction, temperature, max_new_tokens)\u001b[0m\n\u001b[1;32m     65\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 67\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     70\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1553\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1559\u001b[0m     ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1561\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1564\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1553\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1559\u001b[0m     ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1561\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1564\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    967\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    968\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    969\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         cache_position,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1553\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1559\u001b[0m     ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1561\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1564\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:718\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1553\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1559\u001b[0m     ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1561\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1564\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:660\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    658\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 660\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1553\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1559\u001b[0m     ):\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1561\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1564\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:574\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgemv_4bit(A, \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, out, state\u001b[38;5;241m=\u001b[39mquant_state)\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["modelhandler_base = ModelHandler()\n","modelhandler_base.loading_model(model_chosen = 'base_model')\n","base_model_generated_answers = []\n","ground_truths_list = []\n","\n","for triple_with_output, triple_without_output in zip(test_dataset, new_test_dataset):\n","\n","    print(rf'Running triple {triple_with_output}')\n","\n","    modelhandler_base.ask_question(triple_without_output)\n","    base_model_generated_answer = modelhandler_base.parse_output()\n","    ground_truth = triple_with_output['Output']\n","\n","    base_model_generated_answers.append(base_model_generated_answer)\n","    ground_truths_list.append(ground_truth)\n","    \n","rouge_score_base_model = calculate_rouge_scores(base_model_generated_answers,ground_truths_list)\n","rouge_score_base_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Instruction</th>\n","      <th>Input</th>\n","      <th>Output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Summarize the main contribution of LoRA-Guard ...</td>\n","      <td>\"We introduce LoRA-Guard, a parameter-efficien...</td>\n","      <td>\"LoRA-Guard is a parameter-efficient method fo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Explain the limitation of existing model-based...</td>\n","      <td>\"Existing model-based guardrails have not been...</td>\n","      <td>\"Existing model-based guardrails are not suita...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Describe the advantage of LoRA-Guard over exis...</td>\n","      <td>\"We show that LoRA-Guard outperforms existing ...</td>\n","      <td>\"LoRA-Guard achieves better performance than e...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Describe the main idea behind LoRA-Guard.</td>\n","      <td>\"LoRA-Guard uses a low-rank adapter on a backb...</td>\n","      <td>\"LoRA-Guard integrates a chat model and a guar...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Explain the advantage of the dual path design ...</td>\n","      <td>\"The dual path design of LoRA-Guard, based on ...</td>\n","      <td>\"LoRA-Guard's dual path design prevents perfor...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2424</th>\n","      <td>List the dataset name for the given translatio...</td>\n","      <td>\"task1111 ted translation he it\"</td>\n","      <td>\"TED\"\\n\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2425</th>\n","      <td>Identify the source and target languages for t...</td>\n","      <td>\"task1020 pib translation telugu oriya\"</td>\n","      <td>\"Telugu-Oriya\"</td>\n","    </tr>\n","    <tr>\n","      <th>2426</th>\n","      <td>Identify the task type of the given dataset.</td>\n","      <td>\"task1340 msr text compression\"</td>\n","      <td>\"Sentence Compression\"\\n\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2427</th>\n","      <td>List the datasets related to summarization tasks.</td>\n","      <td>\"Table A1\"</td>\n","      <td>\"xlsum, amazon and yelp summarization dataset,...</td>\n","    </tr>\n","    <tr>\n","      <th>2428</th>\n","      <td>Provide an example of a title generation task.</td>\n","      <td>\"Table A2\"</td>\n","      <td>\"task1540 parsed pdfs summarization\"</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2429 rows × 3 columns</p>\n","</div>"],"text/plain":["                                            Instruction  \\\n","0     Summarize the main contribution of LoRA-Guard ...   \n","1     Explain the limitation of existing model-based...   \n","2     Describe the advantage of LoRA-Guard over exis...   \n","3             Describe the main idea behind LoRA-Guard.   \n","4     Explain the advantage of the dual path design ...   \n","...                                                 ...   \n","2424  List the dataset name for the given translatio...   \n","2425  Identify the source and target languages for t...   \n","2426       Identify the task type of the given dataset.   \n","2427  List the datasets related to summarization tasks.   \n","2428     Provide an example of a title generation task.   \n","\n","                                                  Input  \\\n","0     \"We introduce LoRA-Guard, a parameter-efficien...   \n","1     \"Existing model-based guardrails have not been...   \n","2     \"We show that LoRA-Guard outperforms existing ...   \n","3     \"LoRA-Guard uses a low-rank adapter on a backb...   \n","4     \"The dual path design of LoRA-Guard, based on ...   \n","...                                                 ...   \n","2424                   \"task1111 ted translation he it\"   \n","2425            \"task1020 pib translation telugu oriya\"   \n","2426                    \"task1340 msr text compression\"   \n","2427                                         \"Table A1\"   \n","2428                                         \"Table A2\"   \n","\n","                                                 Output  \n","0     \"LoRA-Guard is a parameter-efficient method fo...  \n","1     \"Existing model-based guardrails are not suita...  \n","2     \"LoRA-Guard achieves better performance than e...  \n","3     \"LoRA-Guard integrates a chat model and a guar...  \n","4     \"LoRA-Guard's dual path design prevents perfor...  \n","...                                                 ...  \n","2424                                          \"TED\"\\n\\n  \n","2425                                     \"Telugu-Oriya\"  \n","2426                         \"Sentence Compression\"\\n\\n  \n","2427  \"xlsum, amazon and yelp summarization dataset,...  \n","2428               \"task1540 parsed pdfs summarization\"  \n","\n","[2429 rows x 3 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["json_path = '/workspace/llama3_8b_finetuning/data/arvix_instruction_dataset.json'\n","import pandas as pd\n","import json\n","json_path = '/workspace/llama3_8b_finetuning/data/arvix_instruction_dataset.json'\n","\n","with open(json_path, 'r') as file:\n","    json_data = json.load(file)\n","\n","\n","df = pd.json_normalize(json_data)\n","\n","df\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df.to_csv('./csv_instructions')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["df = pd.read_csv('./csv_instructions').rename(columns = {'Instruction':'instruction', 'Input':'input', 'Output':'output'})\n","df[['instruction','input','output']].to_csv('./csv_instructions',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Summarize the main contribution of LoRA-Guard ...</td>\n","      <td>\"LoRA-Guard is a parameter-efficient method fo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Explain the limitation of existing model-based...</td>\n","      <td>\"Existing model-based guardrails are not suita...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Describe the advantage of LoRA-Guard over exis...</td>\n","      <td>\"LoRA-Guard achieves better performance than e...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Describe the main idea behind LoRA-Guard.</td>\n","      <td>\"LoRA-Guard integrates a chat model and a guar...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Explain the advantage of the dual path design ...</td>\n","      <td>\"LoRA-Guard's dual path design prevents perfor...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2424</th>\n","      <td>List the dataset name for the given translatio...</td>\n","      <td>\"TED\"\\n\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2425</th>\n","      <td>Identify the source and target languages for t...</td>\n","      <td>\"Telugu-Oriya\"</td>\n","    </tr>\n","    <tr>\n","      <th>2426</th>\n","      <td>Identify the task type of the given dataset.</td>\n","      <td>\"Sentence Compression\"\\n\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2427</th>\n","      <td>List the datasets related to summarization tasks.</td>\n","      <td>\"xlsum, amazon and yelp summarization dataset,...</td>\n","    </tr>\n","    <tr>\n","      <th>2428</th>\n","      <td>Provide an example of a title generation task.</td>\n","      <td>\"task1540 parsed pdfs summarization\"</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2429 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  \\\n","0     Summarize the main contribution of LoRA-Guard ...   \n","1     Explain the limitation of existing model-based...   \n","2     Describe the advantage of LoRA-Guard over exis...   \n","3             Describe the main idea behind LoRA-Guard.   \n","4     Explain the advantage of the dual path design ...   \n","...                                                 ...   \n","2424  List the dataset name for the given translatio...   \n","2425  Identify the source and target languages for t...   \n","2426       Identify the task type of the given dataset.   \n","2427  List the datasets related to summarization tasks.   \n","2428     Provide an example of a title generation task.   \n","\n","                                                 target  \n","0     \"LoRA-Guard is a parameter-efficient method fo...  \n","1     \"Existing model-based guardrails are not suita...  \n","2     \"LoRA-Guard achieves better performance than e...  \n","3     \"LoRA-Guard integrates a chat model and a guar...  \n","4     \"LoRA-Guard's dual path design prevents perfor...  \n","...                                                 ...  \n","2424                                          \"TED\"\\n\\n  \n","2425                                     \"Telugu-Oriya\"  \n","2426                         \"Sentence Compression\"\\n\\n  \n","2427  \"xlsum, amazon and yelp summarization dataset,...  \n","2428               \"task1540 parsed pdfs summarization\"  \n","\n","[2429 rows x 2 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('./csv_instructions')\n","df = df[['instruction', 'output']].rename(columns={'instruction':'text', 'output':'target'})\n","df.to_csv('./csv_instructions2', index = False)\n","df\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["df = pd.read_csv('./csv_instructions2')\n","\n","\n","df['conversation'] = 'human: ' + df['text'] + ' \\n bot: ' + df['target']\n","\n","# Dropping the original columns if needed\n","df = df[['conversation']]\n","\n","df.to_csv('./instruction_single_column_csv', index=False)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>conversation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>human: Summarize the main contribution of LoRA...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>human: Explain the limitation of existing mode...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>human: Describe the advantage of LoRA-Guard ov...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>human: Describe the main idea behind LoRA-Guar...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>human: Explain the advantage of the dual path ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2424</th>\n","      <td>human: List the dataset name for the given tra...</td>\n","    </tr>\n","    <tr>\n","      <th>2425</th>\n","      <td>human: Identify the source and target language...</td>\n","    </tr>\n","    <tr>\n","      <th>2426</th>\n","      <td>human: Identify the task type of the given dat...</td>\n","    </tr>\n","    <tr>\n","      <th>2427</th>\n","      <td>human: List the datasets related to summarizat...</td>\n","    </tr>\n","    <tr>\n","      <th>2428</th>\n","      <td>human: Provide an example of a title generatio...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2429 rows × 1 columns</p>\n","</div>"],"text/plain":["                                           conversation\n","0     human: Summarize the main contribution of LoRA...\n","1     human: Explain the limitation of existing mode...\n","2     human: Describe the advantage of LoRA-Guard ov...\n","3     human: Describe the main idea behind LoRA-Guar...\n","4     human: Explain the advantage of the dual path ...\n","...                                                 ...\n","2424  human: List the dataset name for the given tra...\n","2425  human: Identify the source and target language...\n","2426  human: Identify the task type of the given dat...\n","2427  human: List the datasets related to summarizat...\n","2428  human: Provide an example of a title generatio...\n","\n","[2429 rows x 1 columns]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv('instruction_single_column_csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
