{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:40:34.411961Z","iopub.status.busy":"2024-04-21T10:40:34.411239Z","iopub.status.idle":"2024-04-21T10:40:53.528793Z","shell.execute_reply":"2024-04-21T10:40:53.527602Z","shell.execute_reply.started":"2024-04-21T10:40:34.411930Z"},"trusted":true},"outputs":[],"source":["%%capture\n","\n","!pip install bitsandbytes accelerate peft trl\n","\n","import time\n","from random import randrange, sample, seed\n","\n","import torch\n","import os\n","from datasets import load_dataset\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","\n","seed(42)\n","\n","\n","import torch\n","print(torch.cuda.is_available())\n","\n","use_flash_attention2 = False\n","\n","# Replace attention with flash attention \n","if torch.cuda.get_device_capability()[0] >= 8:\n","    use_flash_attention2 = True\n","\n","print(f\"Using flash attention 2: {use_flash_attention2}\")\n","\n","\n","%%capture\n","\n","if use_flash_attention2:\n","    !pip install flash-attn --no-build-isolation --upgrade\n","\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"json\", data_files=\"/home/srvadm001/nucleo-ia/finetuning/Fine-tuning/llama3_7b/snptee-instruction-dataset.json\", split=\"train\")\n","  \n","def format_instruction(sample):\n","\treturn f\"\"\"    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{sample['instruction']}\n","\n","### Input:\n","{sample['input']}\n","\n","### Response:\n","{sample['output']}\n","\"\"\"\n","\n","from dotenv import load_dotenv\n","import os\n","\n","# Load the .env file\n","load_dotenv()\n","\n","\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","import os\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Hugging Face model id\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","# model_id = \"mistralai/Mistral-7B-v0.1\"\n","\n","# BitsAndBytesConfig int-4 config \n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id, \n","    quantization_config=bnb_config, \n","    use_cache=False, \n","    device_map=\"auto\",\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n","    attn_implementation=\"flash_attention_2\" if use_flash_attention2 else \"sdpa\"\n",")\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Specify the directory where you want to save the model and tokenizer\n","save_directory = \"./saved_model\"\n","\n","# Save the model\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(save_directory)\n","\n","\n","# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        r=64,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","            \"q_proj\",\n","            \"k_proj\",\n","            \"v_proj\",\n","            \"o_proj\",\n","            \"gate_proj\", \n","            \"up_proj\", \n","            \"down_proj\",\n","        ]\n",")\n","\n","# Prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","\n","\n","args = TrainingArguments(\n","    output_dir=\"fine-tuned-snptee\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=6 if use_flash_attention2 else 2, # you can play with the batch size depending on your hardware\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=10,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=use_flash_attention2,\n","    fp16=not use_flash_attention2,\n","    tf32=use_flash_attention2,\n","    max_grad_norm=0.3,\n","    warmup_steps=5,\n","    lr_scheduler_type=\"linear\",\n","    disable_tqdm=False,\n","    report_to=\"none\"\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=2048,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_instruction, \n","    args=args,\n",")\n","\n","\n","# train\n","trainer.train()\n","\n","# save model\n","trainer.save_model()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["instructions ['Summarize the main contribution of LoRA-Guard in content moderation of large language models.', 'Explain the limitation of existing model-based guardrails for content moderation of large language models.', 'Describe the advantage of LoRA-Guard over existing approaches in terms of parameter overhead and accuracy.']\n","inputs ['We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models.', 'Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally.', 'We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.']\n","outputs ['LoRA-Guard is a parameter-efficient method for content moderation of LLMs that adapts language features from LLMs using low-rank adapters.', 'Existing model-based guardrails are not suitable for mobile devices due to resource constraints, limiting their use in local LLM-based applications.']\n"]}],"source":["print('instructions', instructions)\n","print('inputs', inputs)\n","print('outputs', outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["instructions ['Summarize the main contribution of LoRA-Guard in content moderation of large language models.', 'Explain the limitation of existing model-based guardrails for content moderation of large language models.', 'Describe the advantage of LoRA-Guard over existing approaches in terms of parameter overhead and accuracy.']\n","inputs ['We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models.', 'Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally.', 'We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.']\n","outputs ['LoRA-Guard is a parameter-efficient method for content moderation of LLMs that adapts language features from LLMs using low-rank adapters.', 'Existing model-based guardrails are not suitable for mobile devices due to resource constraints, limiting their use in local LLM-based applications.']\n"]}],"source":["print('instructions', instructions)\n","print('inputs', inputs)\n","print('outputs', outputs)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%%capture\n","\n","!pip install bitsandbytes accelerate peft trl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:40:53.531106Z","iopub.status.busy":"2024-04-21T10:40:53.530787Z","iopub.status.idle":"2024-04-21T10:41:12.381544Z","shell.execute_reply":"2024-04-21T10:41:12.380576Z","shell.execute_reply.started":"2024-04-21T10:40:53.531076Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/srvadm001/nucleo-ia/finetuning/Fine-tuning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import time\n","from random import randrange, sample, seed\n","\n","import torch\n","import os\n","from datasets import load_dataset\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","\n","seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:12.694374Z","iopub.status.busy":"2024-04-21T10:41:12.694052Z","iopub.status.idle":"2024-04-21T10:41:12.700121Z","shell.execute_reply":"2024-04-21T10:41:12.699090Z","shell.execute_reply.started":"2024-04-21T10:41:12.694346Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using flash attention 2: False\n"]}],"source":["use_flash_attention2 = False\n","\n","# Replace attention with flash attention \n","if torch.cuda.get_device_capability()[0] >= 8:\n","    use_flash_attention2 = True\n","\n","print(f\"Using flash attention 2: {use_flash_attention2}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:12.701677Z","iopub.status.busy":"2024-04-21T10:41:12.701412Z","iopub.status.idle":"2024-04-21T10:41:12.712393Z","shell.execute_reply":"2024-04-21T10:41:12.711603Z","shell.execute_reply.started":"2024-04-21T10:41:12.701653Z"},"trusted":true},"outputs":[],"source":["%%capture\n","\n","if use_flash_attention2:\n","    !pip install flash-attn --no-build-isolation --upgrade"]},{"cell_type":"code","execution_count":37,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-21T10:41:12.713607Z","iopub.status.busy":"2024-04-21T10:41:12.713350Z","iopub.status.idle":"2024-04-21T10:41:20.191567Z","shell.execute_reply":"2024-04-21T10:41:20.190635Z","shell.execute_reply.started":"2024-04-21T10:41:12.713585Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 64 examples [00:00, 7171.67 examples/s]\n"]}],"source":["# Load dataset from the hub\n","# dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n","from datasets import load_dataset\n","dataset = load_dataset(\"json\", data_files=\"/home/srvadm001/nucleo-ia/finetuning/Fine-tuning/llama3_7b/snptee-instruction-dataset.json\", split=\"train\")\n","\n","# print(f\"Dataset size: {len(dataset)}\")\n","# print(dataset[randrange(len(dataset))])\n","\n","# #Reduce dataset to size N\n","# n_samples = sample(range(len(dataset)), k=1000)\n","# print(f\"First 5 samples: {n_samples[:5]}\")\n","# dataset = dataset.select(n_samples)\n","# print(f\"Reduced dataset size: {len(dataset)}\")"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:20.192868Z","iopub.status.busy":"2024-04-21T10:41:20.192575Z","iopub.status.idle":"2024-04-21T10:41:20.197907Z","shell.execute_reply":"2024-04-21T10:41:20.196871Z","shell.execute_reply.started":"2024-04-21T10:41:20.192842Z"},"trusted":true},"outputs":[],"source":["def format_instruction(sample):\n","\treturn f\"\"\"    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{sample['instruction']}\n","\n","### Input:\n","{sample['input']}\n","\n","### Response:\n","{sample['output']}\n","\"\"\""]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:41:20.199512Z","iopub.status.busy":"2024-04-21T10:41:20.199159Z","iopub.status.idle":"2024-04-21T10:41:20.451315Z","shell.execute_reply":"2024-04-21T10:41:20.450321Z","shell.execute_reply.started":"2024-04-21T10:41:20.199481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","['Resuma o conteúdo da figura 3 do artigo.', 'O que é apresentado na página inicial do Espaço de Gestão do Conhecimento da Gerência de Integração e Acesso?', 'A página inicial do Espaço de Gestão do Conhecimento apresenta o conceito de Gestão do Conhecimento (GC) e um menu com os macroprocessos da Gerência de Integração e Acesso do ONS.']\n","\n","### Input:\n","['Descreva a estrutura da página do macroprocesso de Integração das Instalações de Transmissão ao SIN.', 'Como é estruturada a página do macroprocesso de Integração das Instalações de Transmissão ao SIN no Espaço de GC?', 'A página apresenta a apresentação do macroprocesso selecionado, com links rápidos para conteúdos relacionados, e botões de acesso às práticas de GC de Mapeamento de Processos, Checklists e Lições Aprendidas.']\n","\n","### Response:\n","['Explique a abordagem utilizada para mapear os macroprocessos de integração de instalações ao SIN.', 'Qual abordagem foi utilizada para mapear os macroprocessos de integração de instalações ao SIN?', 'A abordagem utilizada foi a metodologia BPM (Business Process Management), utilizando a ferramenta BPMS (Business Process Management System), no caso, o software Bizagi.']\n","\n"]}],"source":["print(format_instruction(dataset[randrange(len(dataset))]))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","import os\n","\n","# Load the .env file\n","load_dotenv()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.56s/it]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["('./saved_model/tokenizer_config.json',\n"," './saved_model/special_tokens_map.json',\n"," './saved_model/tokenizer.json')"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","import os\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Hugging Face model id\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","# model_id = \"mistralai/Mistral-7B-v0.1\"\n","\n","# BitsAndBytesConfig int-4 config \n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16 if use_flash_attention2 else torch.float16\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id, \n","    quantization_config=bnb_config, \n","    use_cache=False, \n","    device_map=\"auto\",\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n","    attn_implementation=\"flash_attention_2\" if use_flash_attention2 else \"sdpa\"\n",")\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    token=os.environ[\"HF_TOKEN\"],  # if model is gated like llama or mistral\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Specify the directory where you want to save the model and tokenizer\n","save_directory = \"./saved_model\"\n","\n","# Save the model\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(save_directory)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:19.246738Z","iopub.status.busy":"2024-04-21T10:46:19.246447Z","iopub.status.idle":"2024-04-21T10:46:19.274061Z","shell.execute_reply":"2024-04-21T10:46:19.273257Z","shell.execute_reply.started":"2024-04-21T10:46:19.246713Z"},"trusted":true},"outputs":[],"source":["# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        r=64,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","            \"q_proj\",\n","            \"k_proj\",\n","            \"v_proj\",\n","            \"o_proj\",\n","            \"gate_proj\", \n","            \"up_proj\", \n","            \"down_proj\",\n","        ]\n",")\n","\n","# Prepare model for training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:19.275606Z","iopub.status.busy":"2024-04-21T10:46:19.275240Z","iopub.status.idle":"2024-04-21T10:46:21.696131Z","shell.execute_reply":"2024-04-21T10:46:21.695089Z","shell.execute_reply.started":"2024-04-21T10:46:19.275574Z"},"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=\"fine-tuned-snptee\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=6 if use_flash_attention2 else 2, # you can play with the batch size depending on your hardware\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=10,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=use_flash_attention2,\n","    fp16=not use_flash_attention2,\n","    tf32=use_flash_attention2,\n","    max_grad_norm=0.3,\n","    warmup_steps=5,\n","    lr_scheduler_type=\"linear\",\n","    disable_tqdm=False,\n","    report_to=\"none\"\n",")\n","\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:21.697662Z","iopub.status.busy":"2024-04-21T10:46:21.697367Z","iopub.status.idle":"2024-04-21T10:46:22.669641Z","shell.execute_reply":"2024-04-21T10:46:22.668685Z","shell.execute_reply.started":"2024-04-21T10:46:21.697637Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 10 examples [00:00, 242.20 examples/s]\n","/home/srvadm001/nucleo-ia/finetuning/Fine-tuning/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  self.dataloader_config.dispatch_batches = dispatch_batches\n","Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"]}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=2048,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_instruction, \n","    args=args,\n",")"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T10:46:22.671049Z","iopub.status.busy":"2024-04-21T10:46:22.670741Z","iopub.status.idle":"2024-04-21T11:13:51.648998Z","shell.execute_reply":"2024-04-21T11:13:51.648127Z","shell.execute_reply.started":"2024-04-21T10:46:22.671021Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/srvadm001/nucleo-ia/finetuning/Fine-tuning/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:02, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# train\n","trainer.train()\n","\n","# save model\n","trainer.save_model()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T11:13:51.651118Z","iopub.status.busy":"2024-04-21T11:13:51.650526Z","iopub.status.idle":"2024-04-21T11:14:54.059744Z","shell.execute_reply":"2024-04-21T11:14:54.058777Z","shell.execute_reply.started":"2024-04-21T11:13:51.651090Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Instruction generated from finetuned model | Inference time - 80.98s:\n","    \n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Responda as seguinte pergunta\n","\n","### Input:\n","Quais os desafios para reduzir as transmissões do efeito estufa?\n","\n","### Response:\n","\n","Os desafios para reduzir as transmissões do efeito estufa são:\n","\n","- Aumento da temperatura global\n","- Mudanças no clima\n","- Mudanças no ciclo hidrológico\n","- Mudanças na distribuição de chuvas\n","- Mudanças na distribuição de chuvas\n","- Mudanças na distribuição de chuvas\n","- Mudanças na distribuição de chuvas\n","- Mudanças na distribuição de chu\n"]}],"source":["if False:\n","    # Path to finetuned model\n","    finetuned_model_dir=\"/kaggle/working/mistral-int4-alpaca\"\n","\n","    # Load finetuned LLM model and tokenizer\n","    model = AutoPeftModelForCausalLM.from_pretrained(\n","        finetuned_model_dir,\n","        low_cpu_mem_usage=True,\n","        torch_dtype=torch.float16,\n","        load_in_4bit=True,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n","\n","instruction = {\n","    \"instruction\": \"Responda as seguinte pergunta\",\n","    \"input\": \"Quais os desafios para reduzir as transmissões do efeito estufa?\",\n","    \"output\": \"\"\n","}\n","prompt = format_instruction(instruction)\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","start_time = time.time()\n","with torch.inference_mode():\n","    outputs = model.generate(input_ids=input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=True, top_p=0.5,temperature=0.5)\n","end_time = time.time()\n","total_time = end_time - start_time\n","output_length = len(outputs[0])-len(input_ids[0])\n","\n","print(f\"\\nInstruction generated from finetuned model | Inference time - {total_time:.2f}s:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["'    \\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nResponda as seguinte pergunta\\n\\n### Input:\\nQuais as variáveis usadas para avaliar a satisfação dos clientes?\\n\\n### Response:\\n\\n'"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["\n","instruction = {\n","    \"instruction\": \"Responda as seguinte pergunta\",\n","    \"input\": \"Quais as variáveis usadas para avaliar a satisfação dos clientes?\",\n","    \"output\": \"\"\n","}\n","prompt = format_instruction(instruction)\n","\n","prompt"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
